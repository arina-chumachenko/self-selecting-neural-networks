{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760aee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1./(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        # using He initialization\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * \\\n",
    "                                   np.sqrt(2. / layers_dims[l - 1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    #print type(cache), len(cache)\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    #m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "###########\n",
    "def add_del_neurons(parameters, del_threshold=0.05, prob_del=0.05, prob_add=0.01, \n",
    "                    max_hidden_size=300, num_below_margin=1):\n",
    "    \"\"\"\n",
    "    Deletes and/or adds hidden layer neurons at the end of each epoch\n",
    "    \"\"\"\n",
    "    assert len(parameters) == 2+2, \\\n",
    "    'self-selecting MLP only works with 1 hidden layer currently'\n",
    "    \n",
    "    Wxh = parameters['W1']\n",
    "    Why = parameters['W2']\n",
    "    bh = parameters['b1']\n",
    "    num_features = Wxh.shape[1]\n",
    "    num_labels = Why.shape[0]\n",
    "    normz = (np.sum(np.abs(Why), axis = 0)) *.5\n",
    "    selected = (np.abs(normz) > del_threshold)\n",
    "    hidden_size = Wxh.shape[0]\n",
    "    \n",
    "    # deleting neurons\n",
    "    if np.sum(selected) < hidden_size - num_below_margin:\n",
    "        deletable = np.where(selected==False)[0]\n",
    "        np.random.shuffle(deletable)\n",
    "        for xx in range(num_below_margin):\n",
    "            selected[deletable[xx]] = True\n",
    "        deletable = deletable[num_below_margin:]\n",
    "        for x in deletable:\n",
    "            if np.random.rand() > prob_del:\n",
    "                selected[x] = True\n",
    "    \n",
    "    if np.sum(selected) < hidden_size:\n",
    "        print('neuron deleted')\n",
    "            \n",
    "    hidden_size = np.sum(selected)\n",
    "    \n",
    "    Wxh = Wxh[selected,:]\n",
    "    normz = normz[selected]\n",
    "    Why = Why[:,selected]\n",
    "    bh = bh[selected]\n",
    "    #need memory terms if updated per mini-batch iter instead of per epoch\n",
    "    \n",
    "    # adding neurons\n",
    "    if hidden_size < max_hidden_size-1:\n",
    "        if ( np.sum(np.abs(normz) > del_threshold) ) > hidden_size - num_below_margin \\\n",
    "            and ( np.random.rand() < prob_add ) or ( np.random.rand() < 1e-4 ):\n",
    "            Wxh = np.append(Wxh, 0.01*np.random.randn(1,num_features), axis=0)\n",
    "            \n",
    "            new_Why = np.random.randn(num_labels,1)\n",
    "            new_Why = .5*del_threshold*new_Why / (1e-8 + np.sum(np.abs(new_Why)))\n",
    "            Why = np.append(Why, new_Why, axis=1)\n",
    "            \n",
    "            bh = np.append(bh, 0)\n",
    "            bh = bh.reshape(bh.shape[0],1)\n",
    "            \n",
    "            # also need memory terms here if updating per mini-batch\n",
    "            if Wxh.shape[0] > hidden_size:\n",
    "                print('neuron added')\n",
    "            \n",
    "            hidden_size += 1\n",
    "          \n",
    "    parameters['W1'] = Wxh\n",
    "    parameters['W2'] = Why\n",
    "    parameters['b1'] = bh\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "###########\n",
    "\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.009, num_iterations=1000, print_cost=True):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Add / delete neurons\n",
    "        #parameters = add_del_neurons(parameters)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    if print_cost:\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    #n = len(parameters) // 2 # number of layers in the neural network\n",
    "    preds = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    yhat, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, yhat.shape[1]):\n",
    "        if yhat[0,i] > 0.5:\n",
    "            preds[0,i] = 1\n",
    "        else:\n",
    "            preds[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((preds == y)*1./m)))\n",
    "        \n",
    "    return preds\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_size = 1000\n",
    "    num_features = 10\n",
    "    \n",
    "    X = np.random.rand(num_features,data_size)\n",
    "    y = np.random.randint(0,2,data_size).reshape(1,data_size)\n",
    "    \n",
    "    layers_dims = [X.shape[0], 10, 10, 1]\n",
    "    parameters = L_layer_model(X, y, layers_dims, num_iterations=10, print_cost=False)\n",
    "    preds = predict(X,y,parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb7fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various utility functions for self-selecting neural net implementations\n",
    "# Notation used mostly follows Andrew Ng's deeplearning.ai course\n",
    "# Author: Ryan Kingery (rkinger@g.clemson.edu)\n",
    "# Last Updated: April 2018\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# TODO: lr finder, batch size increases, lr decay, autodiff\n",
    "\n",
    "# Functions contained:\n",
    "#   sigmoid\n",
    "#   tanh\n",
    "#   relu\n",
    "#   softmax\n",
    "#   initialize_parameters\n",
    "#   forwardprop\n",
    "#   compute_loss\n",
    "#   backprop\n",
    "#   gradient_descent\n",
    "#   random_mini_batches\n",
    "#   initialize_momentum\n",
    "#   momentum\n",
    "#   initialize_adam\n",
    "#   adam\n",
    "#   MLP\n",
    "#   StochasticMLP\n",
    "#   predict\n",
    "#   score\n",
    "\n",
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import logsumexp\n",
    "from scipy.signal import lfilter\n",
    "# from ss_functions import *\n",
    "#np.random.seed(42)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid function a = 1/(1+exp(-z))\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    output of sigmoid(z) function, same shape as Z\n",
    "    \"\"\"   \n",
    "    return 1./(1+np.exp(-Z))\n",
    "\n",
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Implements the hyperbolic tangent function a = tanh(z)\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    output of tanh(z) function, same shape as Z\n",
    "    \"\"\"   \n",
    "    return np.tanh(Z)\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implements the RELU function a = max(0,z)\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of relu(Z) function, same shape as Z\n",
    "    \"\"\"        \n",
    "    return np.maximum(0.,Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Numerically stable implementation of the softmax function a = exp(z)/sum(exp(z))\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    output of softmax(z) function, same shape as Z\n",
    "    \"\"\"   \n",
    "    logA = Z - logsumexp(Z,axis=0).reshape(1,Z.shape[1])\n",
    "    return np.exp(logA)\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- list containing the dimensions of each layer in network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dict of parameters W1, b1, ..., WL, bL:\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)-1            # number of layers in the network\n",
    "    for l in range(1, L+1):\n",
    "        # using He initialization\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * \\\n",
    "                                   np.sqrt(2. / layer_dims[l - 1])\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))        \n",
    "    return parameters\n",
    "\n",
    "def forwardprop(X, parameters, problem_type):\n",
    "    \"\"\"\n",
    "    Implements forward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters function\n",
    "    problem_type -- binary classification, regression, multiclass classification\n",
    "                    (binary, regression, multiclass)\n",
    "    \n",
    "    Returns:\n",
    "    yhat -- last post-activation value\n",
    "    inputs -- dict of inputs containing:\n",
    "                input data X\n",
    "                every Z=W*A+b, for each l=1,...,L\n",
    "                every A=activation(Z), for each l=1,...,L\n",
    "    \"\"\"\n",
    "    inputs = {'A0':X}\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        W = parameters['W'+str(l)]\n",
    "        b = parameters['b'+str(l)]\n",
    "        Z = W.dot(A_prev) + b\n",
    "        A = relu(Z)\n",
    "        inputs['Z'+str(l)] = Z\n",
    "        inputs['A'+str(l)] = A\n",
    "    A_prev = A\n",
    "    W = parameters['W'+str(L)]\n",
    "    b = parameters['b'+str(L)]\n",
    "    Z = W.dot(A_prev) + b\n",
    "    if problem_type == 'regression':\n",
    "        A = Z\n",
    "    elif problem_type == 'binary':\n",
    "        A = sigmoid(Z)\n",
    "        A = np.clip(A,1e-8,1.-1e-8)     # clip to prevent loss from blowing up\n",
    "    elif problem_type == 'multiclass':\n",
    "        A = softmax(Z)\n",
    "        A = np.clip(A,1e-8,1.-1e-8)     # clip to prevent loss from blowing up\n",
    "    inputs['Z'+str(L)] = Z\n",
    "    inputs['A'+str(L)] = A\n",
    "    yhat = A\n",
    "    return yhat,inputs\n",
    "\n",
    "def flatten_weights(parameters):\n",
    "    L = len(parameters) // 2\n",
    "    w = np.array([])\n",
    "    for l in range(1,L+1):\n",
    "        w = np.append(w,parameters['W'+str(l)].flatten())\n",
    "    return w\n",
    "\n",
    "def compute_loss(yhat, y, parameters, reg_param, problem_type):\n",
    "    \"\"\"\n",
    "    Compute average loss over dataset\n",
    "\n",
    "    Arguments:\n",
    "    activations -- dictionary of all activations from forward propagation\n",
    "    y -- true \"label\" vector (e.g. 0 if non-cat, 1 if cat), shape (1, num examples)\n",
    "    problem_type -- binary classification, regression, multiclass classification\n",
    "                    (binary, regression, multiclass)\n",
    "\n",
    "    Returns:\n",
    "    loss -- cross-entropy loss\n",
    "    \"\"\"   \n",
    "    m = y.shape[1]\n",
    "    # calculate base loss\n",
    "    if problem_type == 'regression':\n",
    "        loss = 1./(2*m)*np.sum((y-yhat)**2)\n",
    "    elif problem_type == 'binary':\n",
    "        loss = 1./m*(-np.dot(y,np.log(yhat).T)-np.dot(1-y, np.log(1-yhat).T))\n",
    "    elif problem_type == 'multiclass':\n",
    "        loss = -1./m*np.sum(np.sum(y*np.log(yhat),axis=0))\n",
    "    loss = np.squeeze(loss)      # turns [[17]] into 17).\n",
    "    \n",
    "    # add L1 regularization term\n",
    "    w = flatten_weights(parameters)\n",
    "    loss += 1./m*reg_param*np.sum(np.abs(w))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def backprop(yhat, y, inputs, parameters, problem_type):\n",
    "    \"\"\"\n",
    "    Implements backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    yhat -- probability vector, output of the forward propagation\n",
    "    y -- true \"label\" vector (e.g. 0 if non-cat, 1 if cat)\n",
    "    inputs -- dict of inputs outputted from forward propagation:\n",
    "    parameters -- dict of parameter weights and biases\n",
    "    problem_type -- binary classification, regression, multiclass classification\n",
    "                    (binary, regression, multiclass)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = d loss / dA\n",
    "             grads[\"dW\" + str(l)] = d loss / dW\n",
    "             grads[\"db\" + str(l)] = d loss / db\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(parameters) // 2\n",
    "    y = y.reshape(yhat.shape)\n",
    " \n",
    "    Z = inputs['Z'+str(L)]\n",
    "    A_prev = inputs['A'+str(L-1)]\n",
    "    W = parameters['W'+str(L)]\n",
    "    \n",
    "    if problem_type == 'regression':\n",
    "        m = y.shape[1]\n",
    "        dyhat = -1./m * (y-yhat)          \n",
    "        m = A_prev.shape[1]           \n",
    "        dZ = dyhat\n",
    "    elif problem_type == 'binary':\n",
    "        dyhat = - (np.divide(y,yhat) - np.divide(1-y, 1-yhat))\n",
    "        m = A_prev.shape[1]\n",
    "        dZ = dyhat*sigmoid(Z)*(1-sigmoid(Z))\n",
    "    elif problem_type == 'multiclass':\n",
    "        m = A_prev.shape[1]  \n",
    "        dZ = yhat-y\n",
    "        \n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    dW = 1./m*np.dot(dZ,A_prev.T)\n",
    "    db = 1./m*np.sum(dZ,axis=1,keepdims=True)\n",
    "\n",
    "    grads['dA'+str(L-1)] = dA_prev\n",
    "    grads['dW'+str(L)] = dW\n",
    "    grads['db'+str(L)] = db\n",
    "    \n",
    "    for l in reversed(range(1,L)):\n",
    "        Z = inputs['Z'+str(l)]\n",
    "        A_prev = inputs['A'+str(l-1)]\n",
    "        W = parameters['W'+str(l)]\n",
    "        m = A_prev.shape[1]\n",
    "        \n",
    "        dZ = np.array(dA_prev, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        dA_prev = np.dot(W.T,dZ)\n",
    "        dW = 1./m*np.dot(dZ,A_prev.T)\n",
    "        db = 1./m*np.sum(dZ,axis=1,keepdims=True)\n",
    "\n",
    "        grads['dA'+str(l-1)] = dA_prev\n",
    "        grads['dW'+str(l)] = dW\n",
    "        grads['db'+str(l)] = db\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def gradient_descent(parameters, grads, lr, reg_param, data_size):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- dict containing network parameters \n",
    "    grads -- dict containing gradients backprop\n",
    "    lr -- learning rate for gradient descent (default=0.001)\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = Wl \n",
    "                  parameters[\"b\" + str(l)] = bl\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(L):        \n",
    "        parameters['W'+str(l+1)] -= lr*(grads['dW'+str(l+1)]) #+ \n",
    "                  #(1./data_size)*reg_param*np.sign(parameters['W'+str(l+1)]))\n",
    "        parameters['b'+str(l+1)] -= lr*grads['db'+str(l+1)]                \n",
    "    return parameters\n",
    "\n",
    "def random_mini_batches(X, y, batch_size, seed):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (num features, data size)\n",
    "    y -- true labels, of shape (num labels, data size)\n",
    "    batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    m = y.shape[1]\n",
    "    k = y.shape[0]\n",
    "    mini_batches = []\n",
    "    \n",
    "    #if batch_size >= m:\n",
    "    #    mini_batches = [(X,y)]\n",
    "    if True:#else:\n",
    "        # Step 1: Shuffle (X, Y)\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_y = y[:, permutation].reshape((k,m))\n",
    "    \n",
    "        # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "        num_complete_minibatches = int(np.floor(m*1./batch_size)) # number of batches in partition\n",
    "        for k in range(0, num_complete_minibatches):\n",
    "            mini_batch_X = shuffled_X[:, k * batch_size : (k+1) * batch_size]\n",
    "            mini_batch_y = shuffled_y[:, k * batch_size : (k+1) * batch_size]\n",
    "            mini_batch = (mini_batch_X, mini_batch_y)\n",
    "            mini_batches.append(mini_batch)\n",
    "        \n",
    "        # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        if m % batch_size != 0:\n",
    "            mini_batch_X = shuffled_X[:, num_complete_minibatches * batch_size :]\n",
    "            mini_batch_y = shuffled_y[:, num_complete_minibatches * batch_size :]\n",
    "            mini_batch = (mini_batch_X, mini_batch_y)\n",
    "            mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "\n",
    "def initialize_momentum(parameters):\n",
    "    \"\"\"\n",
    "    Initializes momentum as a dict:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: array of zeros, same size as parameters\n",
    "    Arguments:\n",
    "    parameters -- dict of parameters\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    \n",
    "    Returns:\n",
    "    m -- dict of momentum values\n",
    "                    m['dW' + str(l)] = momentum of dWl\n",
    "                    m['db' + str(l)] = momentum of dbl\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    m = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        m['dW'+str(l+1)] = np.zeros(parameters['W'+str(l+1)].shape)\n",
    "        m['db'+str(l+1)] = np.zeros(parameters['b'+str(l+1)].shape)\n",
    "        \n",
    "    return m\n",
    "\n",
    "def momentum(parameters, grads, m, beta, lr, reg_param, data_size):\n",
    "    \"\"\"\n",
    "    Update parameters using Momentum\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- dict of parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- dict of gradients for each parameter:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    m -- dict of current momentum values:\n",
    "                    m['dW' + str(l)] = ...\n",
    "                    m['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    lr -- the learning rate, scalar\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- dict of parameters \n",
    "    m -- dict of momentum values\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    \n",
    "    for l in range(L):\n",
    "        # compute momentum\n",
    "        m[\"dW\" + str(l+1)] = beta*m[\"dW\" + str(l+1)] + (1-beta)*grads['dW' + str(l+1)]\n",
    "        m[\"db\" + str(l+1)] = beta*m[\"db\" + str(l+1)] + (1-beta)*grads['db' + str(l+1)]\n",
    "        # update parameters\n",
    "        parameters[\"W\" + str(l+1)] -= lr*(m[\"dW\" + str(l+1)]) #+ \n",
    "                  #(1./data_size)*reg_param*np.sign(parameters['W'+str(l+1)]))\n",
    "        parameters[\"b\" + str(l+1)] -= lr*m[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters, m\n",
    "\n",
    "def initialize_adam(parameters) :\n",
    "    \"\"\"\n",
    "    Initializes m and v as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    m -- dict containing exponentially weighted average of the gradient.\n",
    "                    m[\"dW\" + str(l)] = ...\n",
    "                    m[\"db\" + str(l)] = ...\n",
    "    v -- dict containing exponentially weighted average of the squared gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    m = {}\n",
    "    v = {}\n",
    "\n",
    "    for l in range(L):\n",
    "        m[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        m[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "    \n",
    "    return m, v\n",
    "\n",
    "def adam(parameters, grads, m, v, t, lr, beta1, beta2, epsilon, reg_param, data_size):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    m -- Adam variable, moving average of the first gradient moment, dict\n",
    "    v -- Adam variable, moving average of the second gradient moment, dict\n",
    "    lr -- the learning rate, scalar\n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    m -- Adam variable, moving average of the first gradient moment, dict\n",
    "    v -- Adam variable, moving average of the second gradient moment, dict\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2         # number of layers\n",
    "    m_corrected = {}\n",
    "    v_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients\n",
    "        m[\"dW\" + str(l+1)] = beta1*m[\"dW\" + str(l+1)] + (1-beta1)*grads['dW' + str(l+1)]\n",
    "        m[\"db\" + str(l+1)] = beta1*m[\"db\" + str(l+1)] + (1-beta1)*grads['db' + str(l+1)]\n",
    "        #print np.all((m[\"dW\" + str(l+1)] - grads['dW' + str(l+1)]) < 1e-5)\n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_corrected[\"dW\" + str(l+1)] = m[\"dW\" + str(l+1)]/(1-beta1**t)\n",
    "        m_corrected[\"db\" + str(l+1)] = m[\"db\" + str(l+1)]/(1-beta1**t)\n",
    "\n",
    "        # Moving average of the squared gradients\n",
    "        v[\"dW\" + str(l+1)] = beta2*v[\"dW\" + str(l+1)] + (1-beta2)*np.power(grads['dW' + str(l+1)],2)\n",
    "        v[\"db\" + str(l+1)] = beta2*v[\"db\" + str(l+1)] + (1-beta2)*np.power(grads['db' + str(l+1)],2)\n",
    "        #print np.all((v[\"dW\" + str(l+1)] - grads['dW' + str(l+1)]) < 1e-5)\n",
    "        # Compute bias-corrected second raw moment estimate\n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-beta2**t)\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-beta2**t)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters[\"W\" + str(l+1)] -= lr*(np.divide(m_corrected[\"dW\" + str(l+1)],\n",
    "                  np.sqrt(v_corrected[\"dW\" + str(l+1)])+epsilon)) #+ \n",
    "                  #(1./data_size)*reg_param*np.sign(parameters['W'+str(l+1)]))\n",
    "        parameters[\"b\" + str(l+1)] -= lr*np.divide(m_corrected[\"db\" + str(l+1)],\n",
    "                  np.sqrt(v_corrected[\"db\" + str(l+1)])+epsilon)\n",
    "\n",
    "    return parameters, m, v\n",
    "\n",
    "def MLP(X, y, layer_dims, problem_type, X_test, y_test, lr, num_iters, print_loss, add_del, \n",
    "        reg_param, delta,prob,epsilon,max_hidden_size,tau):\n",
    "        #del_threshold, prob_del, prob_add, max_hidden_size, num_below_margin):\n",
    "    \"\"\"\n",
    "    Implements a L-layer multilayer perceptron (MLP)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num data, num features)\n",
    "    y -- true \"label\" vector (e.g. 0 if cat, 1 if non-cat), of shape (1, num data)\n",
    "    layers_dims -- list containing input size and each layer size, length (num layers + 1).\n",
    "    lr -- learning rate of the gradient descent update rule\n",
    "    num_iters -- number of iterations of the optimization loop\n",
    "    print_loss -- if True, it prints the cost every few steps\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    losses = []                \n",
    "    test_losses = []\n",
    "    all_losses = []\n",
    "    num_neurons = []\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        yhat,inputs = forwardprop(X, parameters, problem_type)\n",
    "        \n",
    "        # Compute cost.\n",
    "        loss = compute_loss(yhat, y, parameters, reg_param, problem_type)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = backprop(yhat, y, inputs, parameters, problem_type)        \n",
    " \n",
    "        # Update parameters.\n",
    "        data_size = y.shape[1]\n",
    "        parameters = gradient_descent(parameters, grads, lr, reg_param, data_size)\n",
    "\n",
    "        all_losses.append(loss)\n",
    "\n",
    "        # Add / delete neurons\n",
    "        if add_del:\n",
    "            num_neuron = parameters['b1'].shape[0]\n",
    "            if i>tau:\n",
    "                #parameters = add_del_neurons_orig(parameters,print_add_del,i,del_threshold, \n",
    "                #                             prob_del,prob_add,max_hidden_size,num_below_margin)\n",
    "                parameters = delete_neurons(parameters,delta,prob)\n",
    "                parameters = add_neurons(parameters,all_losses,epsilon,delta,\n",
    "                                         max_hidden_size,tau,prob)\n",
    "            \n",
    "        if X_test is not None and y_test is not None:\n",
    "            yhat_test,_ = forwardprop(X_test, parameters, problem_type)\n",
    "            test_loss = compute_loss(yhat_test, y_test, parameters, reg_param, problem_type)\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        num_prints = max(1,num_iters // 20)\n",
    "        if print_loss and i % num_prints == 0:\n",
    "            print('Loss after iteration %i: %f' % (i, loss))\n",
    "            if add_del:\n",
    "                print('Number of neurons %i: %d' % (i, num_neuron))\n",
    "            if X_test is not None and y_test is not None:\n",
    "                print (\"Test loss after epoch %i: %f\" %(i, test_loss))\n",
    "                \n",
    "        num_losses = 1#max(1,num_iters // 100)\n",
    "        if i % num_losses == 0:\n",
    "            losses.append(loss)\n",
    "            if add_del:\n",
    "                num_neurons.append(num_neuron)\n",
    "            if X_test is not None and y_test is not None:\n",
    "                    test_losses.append(test_loss)        \n",
    "        \n",
    "        #if i>0 and i%1000 == 0:\n",
    "        #    lr = lr/(1+0.0*i)\n",
    "        #    print('learning rate reduced to %f' % lr)\n",
    "            \n",
    "    # plot the cost\n",
    "#    if print_loss:\n",
    "#        xx = np.linspace(1,num_iters+1,num=num_iters)\n",
    "#        plt.plot(xx,losses,color='blue',label='train')\n",
    "#        if X_test is not None and y_test is not None:\n",
    "#            plt.plot(xx,test_losses,color='red',label='test')\n",
    "#        plt.legend(loc='upper right')\n",
    "#        plt.ylabel('loss')\n",
    "#        plt.xlabel('iterations')\n",
    "#        plt.title('Loss')\n",
    "#        plt.show()\n",
    "#        \n",
    "#    if add_del:\n",
    "#        filt_neurons = lfilter([1.0/50]*50,1,num_neurons)\n",
    "#        xx = np.linspace(1,num_iters+1,num=num_iters)\n",
    "#        plt.plot(xx,filt_neurons,color='green',label='# neurons')\n",
    "#        plt.ylabel('# neurons')\n",
    "#        plt.xlabel('epochs')\n",
    "#        plt.title('Number of neurons')\n",
    "#        plt.show()\n",
    "        \n",
    "    return parameters, losses, test_losses, num_neurons\n",
    "\n",
    "def StochasticMLP(X, y, layer_dims, problem_type, X_test, y_test, optimizer, lr, batch_size,\n",
    "                  beta1, beta2, eps, num_epochs, print_loss, add_del, reg_param,\n",
    "                  delta,prob,epsilon,max_hidden_size,tau):\n",
    "                  #del_threshold, prob_del, prob_add, max_hidden_size, num_below_margin):\n",
    "    \"\"\"\n",
    "    MLP which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (num features, data size)\n",
    "    y -- true \"label\" vector, shape (num classes, data size)\n",
    "    layer_dims -- list, containing the size of each layer\n",
    "    lr -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_loss -- True to print the loss every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- dict of parameters \n",
    "    \"\"\"\n",
    "    #X = X.T\n",
    "    #y = y.T\n",
    "    losses = []\n",
    "    test_losses = []\n",
    "    all_losses = []\n",
    "    num_neurons = []\n",
    "    t = 0                            # counter required for Adam update\n",
    "    seed = 42\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    #num_neuron = parameters['b1'].shape[0]\n",
    "    #num_neurons.append(num_neuron)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"sgd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        m = initialize_momentum(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        m,v = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):        \n",
    "        # Define the random minibatches\n",
    "        seed += 1 # reshuffles the dataset differently after each epoch\n",
    "        minibatches = random_mini_batches(X, y, batch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            minibatch_X, minibatch_y = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            yhat,inputs = forwardprop(minibatch_X, parameters, problem_type)\n",
    "\n",
    "            # Compute cost\n",
    "            loss = compute_loss(yhat, minibatch_y, parameters, reg_param, problem_type)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backprop(yhat, minibatch_y, inputs, parameters, problem_type) \n",
    "\n",
    "            # Update parameters\n",
    "            num_in_batch = minibatch_y.shape[1]\n",
    "            if optimizer == \"sgd\":\n",
    "                parameters = gradient_descent(parameters,grads,lr,reg_param,num_in_batch)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, m = momentum(parameters,grads,m,beta1,lr,reg_param,num_in_batch)\n",
    "            elif optimizer == \"adam\":\n",
    "                t += 1 # Adam counter\n",
    "                parameters, m, v = adam(parameters,grads,m,v,t,lr,beta1,beta2,eps,\n",
    "                                        reg_param,num_in_batch)\n",
    "            all_losses.append(loss)\n",
    "        \n",
    "        # Add / delete neurons\n",
    "        if add_del and i>tau:\n",
    "            if optimizer == 'sgd':\n",
    "                #parameters = add_del_neurons_orig(parameters,print_add_del,i,del_threshold, \n",
    "                #                         prob_del,prob_add,max_hidden_size,num_below_margin)\n",
    "                parameters = delete_neurons(parameters,delta,prob)\n",
    "                parameters = add_neurons(parameters,all_losses,epsilon,delta,\n",
    "                                         max_hidden_size,tau,prob)                \n",
    "            if optimizer == 'adam':\n",
    "                parameters,m,v = delete_neurons_adam(parameters,m,v,delta,prob)\n",
    "                parameters,m,v = add_neurons_adam(parameters,m,v,all_losses,epsilon,max_hidden_size,tau,prob)\n",
    "                #print len(add_neurons_adam(parameters,m,v,all_losses,epsilon,max_hidden_size,tau,prob))\n",
    "            \n",
    "        if X_test is not None and y_test is not None:\n",
    "            minibatches = random_mini_batches(X_test, y_test, batch_size, seed)\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                minibatch_X, minibatch_y = minibatch\n",
    "    \n",
    "                # Forward propagation\n",
    "                yhat_test,_ = forwardprop(minibatch_X, parameters, problem_type)\n",
    "    \n",
    "                # Compute cost\n",
    "                test_loss = compute_loss(yhat_test, minibatch_y, parameters, reg_param, problem_type)\n",
    "        \n",
    "        if add_del:\n",
    "            num_neuron = parameters['b1'].shape[0]\n",
    "        \n",
    "        # Print the every few losses\n",
    "        num_prints = max(1,num_epochs // 20)\n",
    "        if print_loss and i % num_prints == 0:\n",
    "            print (\"Training loss after epoch %i: %f\" %(i, loss))\n",
    "            if add_del:\n",
    "                print (\"Number of neurons %i: %d\" %(i, num_neuron))\n",
    "            if X_test is not None and y_test is not None:\n",
    "                print (\"Test loss after epoch %i: %f\" %(i, test_loss))\n",
    "                \n",
    "        num_losses = max(1,num_epochs // 100)\n",
    "        if i % num_losses == 0:\n",
    "            losses.append(loss)\n",
    "            if add_del:\n",
    "                num_neurons.append(num_neuron)\n",
    "            if X_test is not None and y_test is not None:\n",
    "                    test_losses.append(test_loss)\n",
    "                \n",
    "    # plot the cost\n",
    "    if print_loss:\n",
    "        xx = np.linspace(1,num_epochs+1,num=100)\n",
    "        plt.plot(xx,losses,color='blue',label='train')\n",
    "        if X_test is not None and y_test is not None:\n",
    "            plt.plot(xx,test_losses,color='red',label='test')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.title('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "    if add_del:\n",
    "        xx = np.linspace(1,num_epochs+1,num=100)\n",
    "        plt.plot(xx,num_neurons,color='green',label='# neurons')\n",
    "        plt.ylabel('# neurons')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.title('Number of neurons')\n",
    "        plt.show()\n",
    "\n",
    "    return parameters, losses, test_losses\n",
    "\n",
    "def predict(X, parameters, problem_type):\n",
    "    \"\"\"\n",
    "    Uses neural net parameters to predict labels for input data X\n",
    "    \n",
    "    Arguments:\n",
    "    X -- dataset to predict labels for\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    preds -- predictioned labels for dataset X\n",
    "    \"\"\"   \n",
    "    m = X.shape[1]\n",
    "    yhat,_ = forwardprop(X, parameters, problem_type)\n",
    "    preds = np.zeros(yhat.shape)\n",
    "    if problem_type == 'regression':\n",
    "        preds = yhat\n",
    "    elif problem_type == 'binary':\n",
    "        for i in range(0, yhat.shape[1]):\n",
    "            if yhat[0,i] > 0.5:\n",
    "                preds[0,i] = 1\n",
    "            else:\n",
    "                preds[0,i] = 0 \n",
    "    elif problem_type == 'multiclass':\n",
    "        max_idxs = np.argmax(yhat, axis=0)\n",
    "        for i in range(m):\n",
    "            imax = max_idxs[i]\n",
    "            preds[imax,i] = 1\n",
    "    return preds\n",
    "\n",
    "def score(X, y, parameters, problem_type):\n",
    "    \"\"\"\n",
    "    Calculates accuracy of neural net on inputs X, true labels y\n",
    "    \n",
    "    Arguments:\n",
    "    X -- dataset to predict labels for\n",
    "    y -- true labels for X\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    acc -- num correctly predict labels / num total labels (if classification)\n",
    "           R^2 value (if regression)\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    if problem_type == 'regression':\n",
    "        preds = predict(X,parameters,problem_type)    \n",
    "        ssr = np.sum((y-preds)**2)\n",
    "        sst = np.sum((y-np.mean(y,axis=1))**2)\n",
    "        acc = 1.-ssr/sst\n",
    "    elif problem_type == 'binary':\n",
    "        preds = predict(X,parameters,problem_type)\n",
    "        acc = np.sum((preds == y)*1./m)\n",
    "    elif problem_type == 'multiclass':\n",
    "        yhat,_ = forwardprop(X, parameters, problem_type)\n",
    "        acc = 1./m*np.count_nonzero(np.argmax(yhat, axis=0) == np.argmax(y, axis=0))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b21b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File contains all self-selection functions to add/delete neurons\n",
    "# Notation used mostly follows Andrew Ng's deeplearning.ai course\n",
    "# Author: Ryan Kingery (rkinger@g.clemson.edu)\n",
    "# Last Updated: April 2018\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# Functions contained:\n",
    "#   delete_neurons\n",
    "#   add_neurons\n",
    "#   delete_neurons_adam\n",
    "#   add_neurons_adam\n",
    "#   add_del_neurons_orig\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.signal import lfilter\n",
    "#np.random.seed(42)\n",
    "\n",
    "\n",
    "def delete_neurons(parameters,delta,prob):\n",
    "    \"\"\"\n",
    "    Deletes neurons with small outgoing weights from layer\n",
    "    \"\"\"\n",
    "    assert len(parameters) == 2+2, \\\n",
    "    'self-selecting MLP only works with 1 hidden layer currently'\n",
    "    l = 1   # applying to layer 1, plan to extend to other layers later\n",
    "    \n",
    "    W_out = parameters['W'+str(l+1)]    \n",
    "    hidden_size = W_out.shape[1]\n",
    "    \n",
    "    norms = np.sum(np.abs(W_out),axis=0)\n",
    "    max_out = np.max(norms)\n",
    "    selected = (norms == norms) # initialize all True == keep all neurons\n",
    "    \n",
    "    for j in range(hidden_size):\n",
    "        norm = norms[j]\n",
    "        if (norm < delta*max_out) and (np.random.rand() < prob):\n",
    "            # remove neuron j with probability prob\n",
    "            selected[j] = False\n",
    "    \n",
    "    if np.sum(selected) == 0:\n",
    "        # don't want ALL neurons in layer deleted or training will crash\n",
    "        # keep neuron with largest outgoing weights if this occurs\n",
    "        selected[np.argmax(norms)] = True\n",
    "                    \n",
    "    parameters['W'+str(l)] = parameters['W'+str(l)][selected,:]\n",
    "    parameters['b'+str(l)] = parameters['b'+str(l)][selected,:]\n",
    "    parameters['W'+str(l+1)] = parameters['W'+str(l+1)][:,selected]\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def add_neurons(parameters,losses,epsilon,delta,max_hidden_size,tau,prob):\n",
    "    \"\"\"\n",
    "    Add neuron to bottom of layer if loss is stalling\n",
    "    \"\"\"\n",
    "    assert len(parameters) == 2+2, \\\n",
    "    'self-selecting MLP only works with 1 hidden layer currently'\n",
    "    l = 1   # applying to layer 1, plan to extend to other layers later\n",
    "    \n",
    "    W_in = parameters['W'+str(l)]\n",
    "    b_in = parameters['b'+str(l)]\n",
    "    W_out = parameters['W'+str(l+1)]    \n",
    "    hidden_size = b_in.shape[0]\n",
    "    \n",
    "    if hidden_size >= max_hidden_size:\n",
    "        return parameters\n",
    "    \n",
    "    max_loss = np.max(losses)\n",
    "    filt_losses = lfilter([1.0/5]*5,1,losses) # filter noise with FIR filter\n",
    "    losses = filt_losses[-tau:]  # keep only losses in window t-tau,...,t\n",
    "    upper = np.mean(losses) + epsilon*max_loss\n",
    "    lower = np.mean(losses) - epsilon*max_loss\n",
    "    num_out_of_window = np.logical_or((losses < lower),(losses > upper))\n",
    "    \n",
    "    if (np.sum(num_out_of_window) == 0) and (np.random.rand() < prob):\n",
    "        # if losses in window are too similar, add neuron with probability prob\n",
    "        delta = 0.1#3.*delta\n",
    "        new_W_in = np.random.normal(0,2.*delta,size=(1,W_in.shape[1]))\n",
    "        new_b_in = np.zeros((1,1))\n",
    "        new_W_out = np.random.normal(0,2.*delta,size=(W_out.shape[0],1))\n",
    "        W_in = np.append(W_in, new_W_in, axis=0)\n",
    "        b_in = np.append(b_in, new_b_in, axis=0)\n",
    "        W_out = np.append(W_out, new_W_out, axis=1)    \n",
    "    \n",
    "    parameters['W'+str(l)] = W_in\n",
    "    parameters['b'+str(l)] = b_in\n",
    "    parameters['W'+str(l+1)] = W_out\n",
    "    \n",
    "    return parameters    \n",
    "\n",
    "\n",
    "def delete_neurons_adam(parameters,m,v,delta,prob):\n",
    "    \"\"\"\n",
    "    Deletes neurons with small outgoing weights from layer, for use with Adam\n",
    "    \"\"\"\n",
    "    assert len(parameters) == 2+2, \\\n",
    "    'self-selecting MLP only works with 1 hidden layer currently'\n",
    "    l = 1   # applying to layer 1, plan to extend to other layers later\n",
    "    \n",
    "    W_out = parameters['W'+str(l+1)]    \n",
    "    hidden_size = W_out.shape[1]\n",
    "    \n",
    "    norms = np.sum(np.abs(W_out),axis=0)\n",
    "    selected = (norms == norms) # initialize all True == keep all neurons\n",
    "    \n",
    "    for j in range(hidden_size):\n",
    "        norm = norms[j]\n",
    "        if (norm < delta) and (np.random.rand() < prob):\n",
    "            # remove neuron j with probability prob\n",
    "            selected[j] = False\n",
    "    \n",
    "    if np.sum(selected) == 0:\n",
    "        # don't want ALL neurons in layer deleted or training will crash\n",
    "        # keep neuron with largest outgoing weights if this occurs\n",
    "        selected[np.argmax(norms)] = True\n",
    "                    \n",
    "    parameters['W'+str(l)] = parameters['W'+str(l)][selected,:]\n",
    "    parameters['b'+str(l)] = parameters['b'+str(l)][selected,:]\n",
    "    parameters['W'+str(l+1)] = parameters['W'+str(l+1)][:,selected]\n",
    "\n",
    "    m['dW'+str(l)] = m['dW'+str(l)][selected,:]\n",
    "    m['db'+str(l)] = m['db'+str(l)][selected,:]\n",
    "    m['dW'+str(l+1)] = m['dW'+str(l+1)][:,selected]\n",
    "    \n",
    "    v['dW'+str(l)] = v['dW'+str(l)][selected,:]\n",
    "    v['db'+str(l)] = v['db'+str(l)][selected,:]\n",
    "    v['dW'+str(l+1)] = v['dW'+str(l+1)][:,selected]\n",
    "    \n",
    "    return parameters,m,v\n",
    "\n",
    "\n",
    "def add_neurons_adam(parameters,m,v,losses,epsilon,max_hidden_size,tau,prob):\n",
    "    \"\"\"\n",
    "    Add neuron to bottom of layer if loss is stalling, for use with Adam\n",
    "    \"\"\"\n",
    "    assert len(parameters) == 2+2, \\\n",
    "    'self-selecting MLP only works with 1 hidden layer currently'\n",
    "    l = 1   # applying to layer 1, plan to extend to other layers later\n",
    "    \n",
    "    W_in = parameters['W'+str(l)]\n",
    "    b_in = parameters['b'+str(l)]\n",
    "    W_out = parameters['W'+str(l+1)]    \n",
    "    hidden_size = b_in.shape[0]\n",
    "    \n",
    "    mW_in = m['dW'+str(l)]\n",
    "    mb_in = m['db'+str(l)]\n",
    "    mW_out = m['dW'+str(l+1)]\n",
    "    \n",
    "    vW_in = v['dW'+str(l)]\n",
    "    vb_in = v['db'+str(l)]\n",
    "    vW_out = v['dW'+str(l+1)]\n",
    "\n",
    "    if hidden_size >= max_hidden_size:\n",
    "        return parameters,m,v\n",
    "    \n",
    "    #max_loss = np.max(losses)\n",
    "    filt_losses = lfilter([1.0/5]*5,1,losses) # filter noise with FIR filter\n",
    "    losses = filt_losses[-tau:]  # keep only losses in window t-tau,...,t\n",
    "    upper = np.mean(losses) + epsilon#*max_loss\n",
    "    lower = np.mean(losses) - epsilon#*max_loss\n",
    "    num_out_of_window = np.logical_or((losses < lower),(losses > upper))\n",
    "    \n",
    "    if (np.sum(num_out_of_window) == 0) and (np.random.rand() < prob):\n",
    "        # if losses in window are too similar, add neuron with probability prob\n",
    "        W_in = np.append(W_in, .01*np.random.randn(1,W_in.shape[1]), axis=0)\n",
    "        b_in = np.append(b_in, np.zeros((1,1)), axis=0)\n",
    "        W_out = np.append(W_out, .01*np.random.randn(W_out.shape[0],1), axis=1)\n",
    "        \n",
    "        mW_in = np.append(mW_in, .01*np.ones((1,W_in.shape[1])), axis=0)\n",
    "        mb_in = np.append(mb_in, .01*np.ones((1,1)), axis=0)\n",
    "        mW_out = np.append(mW_out, .01*np.ones((W_out.shape[0],1)), axis=1)\n",
    "\n",
    "        vW_in = np.append(vW_in, .01*np.ones((1,W_in.shape[1])), axis=0)\n",
    "        vb_in = np.append(vb_in, .01*np.ones((1,1)), axis=0)\n",
    "        vW_out = np.append(vW_out, .01*np.ones((W_out.shape[0],1)), axis=1)        \n",
    "    \n",
    "    parameters['W'+str(l)] = W_in\n",
    "    parameters['b'+str(l)] = b_in\n",
    "    parameters['W'+str(l+1)] = W_out\n",
    "    \n",
    "    m['dW'+str(l)] = mW_in\n",
    "    m['db'+str(l)] = mb_in\n",
    "    m['dW'+str(l+1)] = mW_out\n",
    "    \n",
    "    v['dW'+str(l)] = vW_in\n",
    "    v['db'+str(l)] = vb_in\n",
    "    v['dW'+str(l+1)] = vW_out\n",
    "    \n",
    "    return parameters,m,v\n",
    "\n",
    "\n",
    "def add_del_neurons_orig(parameters, itr, del_threshold, prob_del, prob_add, \n",
    "                         max_hidden_size, num_below_margin, print_add_del=False):\n",
    "    \"\"\"\n",
    "    Original add_del_neurons function, closely follows Miconi\n",
    "    Deletes and/or adds hidden layer neurons at the end of each epoch\n",
    "    Arguments:\n",
    "        parameters -- dict of parameters (weights and biases)\n",
    "        print_add_del -- prints if neuron added/deleted if True (boolean)\n",
    "        itr -- iteration of training (positive int)\n",
    "        del_threshold -- threshold value determining neural deletion (>0)\n",
    "        prob_del -- probability of deleting neuron if below threshold (0,...,1)\n",
    "        prob_add -- probability of adding neuron at each iteration (0,...,1)\n",
    "        max_hidden_size -- preferred max size of hidden layer (>0)\n",
    "        num_below_margin -- number of below-threshold neurons not deleted (>0)\n",
    "       \n",
    "    Returns:\n",
    "        parameters -- new dict of parameters with neurons added/deleted\n",
    "    \"\"\"\n",
    "    assert len(parameters) == 2+2, \\\n",
    "    'self-selecting MLP only works with 1 hidden layer currently'\n",
    "    \n",
    "    Wxh = parameters['W1']\n",
    "    Why = parameters['W2']\n",
    "    bh = parameters['b1']\n",
    "    num_features = Wxh.shape[1]\n",
    "    num_labels = Why.shape[0]\n",
    "    normz = (np.sum(np.abs(Why), axis = 0)) *.5\n",
    "    selected = (np.abs(normz) > del_threshold)\n",
    "    hidden_size = Wxh.shape[0]\n",
    "    \n",
    "    # deleting neurons\n",
    "    if np.sum(selected) < hidden_size - num_below_margin:\n",
    "        deletable = np.where(selected==False)[0]\n",
    "        np.random.shuffle(deletable)\n",
    "        for xx in range(num_below_margin):\n",
    "            selected[deletable[xx]] = True\n",
    "        deletable = deletable[num_below_margin:]\n",
    "        for x in deletable:\n",
    "            if np.random.rand() > prob_del:\n",
    "                selected[x] = True\n",
    "    \n",
    "    if print_add_del and np.sum(selected) < hidden_size:\n",
    "        print('neuron deleted at iteration %d' % itr)\n",
    "            \n",
    "    hidden_size = np.sum(selected)\n",
    "    \n",
    "    Wxh = Wxh[selected,:]\n",
    "    normz = normz[selected]\n",
    "    Why = Why[:,selected]\n",
    "    bh = bh[selected]\n",
    "    #need memory terms if updated per mini-batch iter instead of per epoch\n",
    "    \n",
    "    # adding neurons\n",
    "    if hidden_size < max_hidden_size-1:\n",
    "        if ( np.sum(np.abs(normz) > del_threshold) ) > hidden_size - num_below_margin \\\n",
    "            and ( np.random.rand() < prob_add ) or ( np.random.rand() < 1e-4 ):\n",
    "            Wxh = np.append(Wxh, 0.01*np.random.randn(1,num_features), axis=0)\n",
    "            \n",
    "            new_Why = np.random.randn(num_labels,1)\n",
    "            new_Why = .5*del_threshold*new_Why / (1e-8 + np.sum(np.abs(new_Why)))# + 0.05\n",
    "            Why = np.append(Why, new_Why, axis=1)\n",
    "            \n",
    "            bh = np.append(bh, 0)\n",
    "            bh = bh.reshape(bh.shape[0],1)\n",
    "            \n",
    "            # also need memory terms here if updating per mini-batch\n",
    "            if print_add_del and Wxh.shape[0] > hidden_size:\n",
    "               print('neuron added at iteration %d' % itr)\n",
    "            \n",
    "            hidden_size += 1\n",
    "          \n",
    "    parameters['W1'] = Wxh\n",
    "    parameters['W2'] = Why\n",
    "    parameters['b1'] = bh\n",
    "    #self.hidden_layer_sizes[0] = hidden_size\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219de11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.591406\n",
      "Number of neurons 0: 1\n",
      "Test loss after epoch 0: 0.598347\n",
      "Loss after iteration 500: 0.329586\n",
      "Number of neurons 500: 1\n",
      "Test loss after epoch 500: 0.304926\n",
      "Loss after iteration 1000: 0.305038\n",
      "Number of neurons 1000: 1\n",
      "Test loss after epoch 1000: 0.277363\n",
      "Loss after iteration 1500: 0.299268\n",
      "Number of neurons 1500: 1\n",
      "Test loss after epoch 1500: 0.270403\n",
      "Loss after iteration 2000: 0.296876\n",
      "Number of neurons 2000: 1\n",
      "Test loss after epoch 2000: 0.267484\n",
      "Loss after iteration 2500: 0.295528\n",
      "Number of neurons 2500: 1\n",
      "Test loss after epoch 2500: 0.265919\n",
      "Loss after iteration 3000: 0.294631\n",
      "Number of neurons 3000: 1\n",
      "Test loss after epoch 3000: 0.265018\n",
      "Loss after iteration 3500: 0.293971\n",
      "Number of neurons 3500: 1\n",
      "Test loss after epoch 3500: 0.264368\n",
      "Loss after iteration 4000: 0.293544\n",
      "Number of neurons 4000: 1\n",
      "Test loss after epoch 4000: 0.263822\n",
      "Loss after iteration 4500: 0.293168\n",
      "Number of neurons 4500: 2\n",
      "Test loss after epoch 4500: 0.263400\n",
      "Loss after iteration 5000: 0.292916\n",
      "Number of neurons 5000: 2\n",
      "Test loss after epoch 5000: 0.263171\n",
      "Loss after iteration 5500: 0.292687\n",
      "Number of neurons 5500: 2\n",
      "Test loss after epoch 5500: 0.263067\n",
      "Loss after iteration 6000: 0.292480\n",
      "Number of neurons 6000: 2\n",
      "Test loss after epoch 6000: 0.262982\n",
      "Loss after iteration 6500: 0.292291\n",
      "Number of neurons 6500: 2\n",
      "Test loss after epoch 6500: 0.262894\n",
      "Loss after iteration 7000: 0.245001\n",
      "Number of neurons 7000: 3\n",
      "Test loss after epoch 7000: 0.220073\n",
      "Loss after iteration 7500: 0.128575\n",
      "Number of neurons 7500: 3\n",
      "Test loss after epoch 7500: 0.119948\n",
      "Loss after iteration 8000: 0.085290\n",
      "Number of neurons 8000: 3\n",
      "Test loss after epoch 8000: 0.081301\n",
      "Loss after iteration 8500: 0.059129\n",
      "Number of neurons 8500: 3\n",
      "Test loss after epoch 8500: 0.056524\n",
      "Loss after iteration 9000: 0.042645\n",
      "Number of neurons 9000: 3\n",
      "Test loss after epoch 9000: 0.040863\n",
      "Loss after iteration 9500: 0.032269\n",
      "Number of neurons 9500: 3\n",
      "Test loss after epoch 9500: 0.030983\n",
      "time = 20.260307\n",
      "training accuracy = 1.000\n",
      "test accuracy = 1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implements a self-selecting multilayer perceptron for binary classification\n",
    "# Notation used mostly follows Andrew Ng's deeplearning.ai course\n",
    "# Author: Ryan Kingery (rkinger@g.clemson.edu)\n",
    "# Last Updated: April 2018\n",
    "# License: BSD 3 clause\n",
    "\n",
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import lfilter\n",
    "import time\n",
    "from utils import *\n",
    "np.random.seed(2)\n",
    "    \n",
    "def BinaryMLP(X, y, layer_dims, X_test=None, y_test=None, lr=0.01, num_iters=1000, \n",
    "                  print_loss=True, add_del=False, reg_param=0., delta=0.1, prob=1., \n",
    "                  epsilon=1e-4, max_hidden_size=100, tau=50):\n",
    "                  #del_threshold=0.03, prob_del=0.05, prob_add=0.05, max_hidden_size=300, num_below_margin=5):\n",
    "    \n",
    "    parameters, losses, test_losses, num_neurons = \\\n",
    "        MLP(X, y, layer_dims, 'binary', X_test, y_test, lr, num_iters, print_loss, add_del, \n",
    "        reg_param, delta,prob,epsilon,max_hidden_size,tau)\n",
    "    return parameters, losses, test_losses, num_neurons\n",
    "\n",
    "def BinaryStochasticMLP(X, y, layer_dims, X_test=None, y_test=None, optimizer='sgd', \n",
    "                  lr=0.01, batch_size=64, beta1=0.9, beta2=0.999, eps=1e-8, \n",
    "                  num_epochs=1000, print_loss=True, add_del=False, reg_param=0.,\n",
    "                  delta=0.03, prob=.5, epsilon=.001, max_hidden_size=100, tau=30):\n",
    "                  #del_threshold=0.03, prob_del=1., prob_add=1., max_hidden_size=300, num_below_margin=1):\n",
    "    \n",
    "    parameters, losses, test_losses = \\\n",
    "        StochasticMLP(X, y, layer_dims, 'binary', X_test, y_test, optimizer, lr, batch_size,\n",
    "                  beta1, beta2, eps, num_epochs, print_loss, add_del, reg_param,\n",
    "                  delta,prob,epsilon,max_hidden_size,tau)\n",
    "    \n",
    "    return parameters, losses, test_losses\n",
    "\n",
    "def gen_data(size=1000,var=2.):\n",
    "    centers = 5\n",
    "    M = [np.random.multivariate_normal(np.array([1,0]),.8*np.eye(2)) for i in range(centers)] +\\\n",
    "        [np.random.multivariate_normal(np.array([0,1]),.8*np.eye(2)) for i in range(centers)]\n",
    "    \n",
    "    X = np.zeros((size,2))\n",
    "    y = np.zeros((size,))\n",
    "    x1 = []\n",
    "    x2 = []    \n",
    "    for j in range(size):\n",
    "        i = np.random.randint(2*centers)\n",
    "        m = M[i]\n",
    "        X[j,:] = np.random.multivariate_normal(np.array(m),var*np.eye(2)/centers)\n",
    "        if i<centers:\n",
    "            y[j] = 0\n",
    "            x1 += [X[j,:]]\n",
    "        else:\n",
    "            y[j] = 1\n",
    "            x2 += [X[j,:]]\n",
    "    x1 = np.array(x1).reshape(len(x1),2)\n",
    "    x2 = np.array(x2).reshape(len(x2),2)\n",
    "    return X,y,x1,x2\n",
    "\n",
    "def plot_model(parameters,x1,x2):\n",
    "    xx, yy = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "    XX = np.c_[xx.ravel(), yy.ravel()].T\n",
    "\n",
    "    yhat = predict(XX,parameters,'binary').reshape(xx.shape)\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.contour(xx, yy, yhat, levels=[.5])#, cmap=\"Greys\", vmin=0, vmax=.6)\n",
    "    \n",
    "    ax.scatter(x1[:,0],x1[:,1],marker='.',c='red',label='y=0')\n",
    "    ax.scatter(x2[:,0],x2[:,1],marker='.',c='blue',label='y=1')\n",
    "    \n",
    "    ax.set(aspect=\"equal\",\n",
    "           xlim=(-3, 3), ylim=(-3, 3),\n",
    "           xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_size = 1000\n",
    "    num_features = 2\n",
    "    \n",
    "    #X = np.random.rand(num_features,data_size)\n",
    "    #y = np.random.randint(0,2,data_size).reshape(1,data_size)\n",
    "    X,y,x1,x2 = gen_data(size=data_size,var=0.01)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    X_train = X_train.T\n",
    "    X_test = X_test.T\n",
    "    y_train = y_train.T.reshape(1,-1)\n",
    "    y_test = y_test.T.reshape(1,-1)\n",
    "    \n",
    "    layer_dims = [X_train.shape[0], 1, 1]\n",
    "    num_iters = 10000\n",
    "    lr = 0.1\n",
    "    #bs = X_train.shape[1] / 16\n",
    "    \n",
    "    tin = time.clock()\n",
    "    parameters,_,ad_loss,num_neurons = \\\n",
    "        BinaryMLP(X_train, y_train, layer_dims, X_test=X_test,\n",
    "                  y_test=y_test, num_iters=num_iters, add_del=True, \n",
    "                  print_loss=True, lr=lr)\n",
    "    tout = time.clock()\n",
    "    tdiff = tout-tin\n",
    "    print('time = %f'% tdiff)\n",
    "    \n",
    "    print('training accuracy = %.3f' % score(X_train,y_train,parameters,'binary'))\n",
    "    print('test accuracy = %.3f' % score(X_test,y_test,parameters,'binary'))\n",
    "    plot_model(parameters,x1,x2)\n",
    "    \n",
    "#    layer_dims = [X_train.shape[0], 10, 1]\n",
    "#    parameters,_,reg_loss,_ = \\\n",
    "#    BinaryMLP(X_train, y_train, layer_dims, X_test=X_test,\n",
    "#              y_test=y_test, num_iters=num_iters, add_del=False, \n",
    "#              print_loss=True, lr=lr)\n",
    "#    print('training accuracy = %.3f' % score(X_train,y_train,parameters,'binary'))\n",
    "#    print('test accuracy = %.3f' % score(X_test,y_test,parameters,'binary'))\n",
    "#    plot_model(parameters,x1,x2)\n",
    "\n",
    "    xx = np.linspace(1,num_iters+1,num=num_iters)\n",
    "    plt.plot(xx,2.*np.max(num_neurons)*np.array(ad_loss),color='blue',label='val loss')\n",
    "    filt_neurons = lfilter([1.0/50]*50,1,num_neurons)\n",
    "    plt.plot(xx,filt_neurons,color='green',label='neurons')\n",
    "    plt.legend(loc='center right')\n",
    "    plt.xlabel('iteration')\n",
    "    #plt.ylabel('loss')\n",
    "    plt.title('Gaussian Mixtures')\n",
    "    plt.show()\n",
    "\n",
    "#    parameters,_,ad_loss = BinaryStochasticMLP(X_train, y_train, layer_dims, X_test=X_test, y_test=y_test, \n",
    "#                                        num_epochs=num_iters, lr=lr, add_del=True, optimizer='adam', \n",
    "#                                        batch_size=bs, print_loss=True)\n",
    "#    print('train accuracy = %.3f' % score(X_train,y_train,parameters,'binary'))\n",
    "#    print('test accuracy = %.3f' % score(X_test,y_test,parameters,'binary'))\n",
    "#    plot_model(parameters,x1,x2)\n",
    "#    \n",
    "#    parameters,_,reg_loss = BinaryStochasticMLP(X_train, y_train, layer_dims, X_test=X_test, y_test=y_test, \n",
    "#                                       num_epochs=num_iters, lr=lr, add_del=False, optimizer='adam', \n",
    "#                                       batch_size=bs, print_loss=True)\n",
    "#    print('train accuracy = %.3f' % score(X_train,y_train,parameters,'binary'))\n",
    "#    print('test accuracy = %.3f' % score(X_test,y_test,parameters,'binary'))\n",
    "\n",
    "#    xx = np.linspace(1,num_iters+1,num=100)\n",
    "#    plt.plot(xx,ad_loss,color='blue',label='add/del')\n",
    "#    plt.plot(xx,reg_loss,color='red',label='regular')\n",
    "#    plt.legend(loc='upper right')\n",
    "#    plt.xlabel('iteration')\n",
    "#    plt.ylabel('loss')\n",
    "#    plt.title('Test Loss')\n",
    "#    plt.show()\n",
    "    \n",
    "    #ps = [0.05, 0.1, 0.15, 0.2, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df08150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import lfilter\n",
    "from utils import *\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "375cbe21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0630243e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 2.454838\n",
      "Number of neurons 0: 1\n",
      "Test loss after epoch 0: 2.362013\n",
      "Loss after iteration 500: 2.013795\n",
      "Number of neurons 500: 1\n",
      "Test loss after epoch 500: 2.013870\n",
      "Loss after iteration 1000: 1.951750\n",
      "Number of neurons 1000: 1\n",
      "Test loss after epoch 1000: 1.957736\n",
      "Loss after iteration 1500: 1.851666\n",
      "Number of neurons 1500: 2\n",
      "Test loss after epoch 1500: 1.862577\n",
      "Loss after iteration 2000: 1.730925\n",
      "Number of neurons 2000: 2\n",
      "Test loss after epoch 2000: 1.740089\n",
      "Loss after iteration 2500: 1.608022\n",
      "Number of neurons 2500: 2\n",
      "Test loss after epoch 2500: 1.612746\n",
      "Loss after iteration 3000: 1.492266\n",
      "Number of neurons 3000: 2\n",
      "Test loss after epoch 3000: 1.499657\n",
      "Loss after iteration 3500: 1.411660\n",
      "Number of neurons 3500: 2\n",
      "Test loss after epoch 3500: 1.427692\n",
      "Loss after iteration 4000: 1.353020\n",
      "Number of neurons 4000: 2\n",
      "Test loss after epoch 4000: 1.379107\n",
      "Loss after iteration 4500: 1.290117\n",
      "Number of neurons 4500: 4\n",
      "Test loss after epoch 4500: 1.326238\n",
      "Loss after iteration 5000: 1.131618\n",
      "Number of neurons 5000: 4\n",
      "Test loss after epoch 5000: 1.171733\n",
      "Loss after iteration 5500: 0.968513\n",
      "Number of neurons 5500: 4\n",
      "Test loss after epoch 5500: 1.008046\n",
      "Loss after iteration 6000: 0.868345\n",
      "Number of neurons 6000: 4\n",
      "Test loss after epoch 6000: 0.904531\n",
      "Loss after iteration 6500: 0.802867\n",
      "Number of neurons 6500: 4\n",
      "Test loss after epoch 6500: 0.836524\n",
      "Loss after iteration 7000: 0.756292\n",
      "Number of neurons 7000: 7\n",
      "Test loss after epoch 7000: 0.791710\n",
      "Loss after iteration 7500: 0.711512\n",
      "Number of neurons 7500: 11\n",
      "Test loss after epoch 7500: 0.746377\n",
      "Loss after iteration 8000: 0.666101\n",
      "Number of neurons 8000: 13\n",
      "Test loss after epoch 8000: 0.705078\n",
      "Loss after iteration 8500: 0.612889\n",
      "Number of neurons 8500: 13\n",
      "Test loss after epoch 8500: 0.655787\n",
      "Loss after iteration 9000: 0.560721\n",
      "Number of neurons 9000: 14\n",
      "Test loss after epoch 9000: 0.604686\n",
      "Loss after iteration 9500: 0.524655\n",
      "Number of neurons 9500: 19\n",
      "Test loss after epoch 9500: 0.575238\n",
      "training accuracy = 0.852\n",
      "test accuracy = 0.832\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implements a self-selecting multilayer perceptron for multiclass classification\n",
    "# Notation used mostly follows Andrew Ng's deeplearning.ai course\n",
    "# Author: Ryan Kingery (rkinger@g.clemson.edu)\n",
    "# Last Updated: April 2018\n",
    "# License: BSD 3 clause\n",
    "\n",
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import lfilter\n",
    "from utils import *\n",
    "np.random.seed(42)\n",
    "\n",
    "def MulticlassMLP(X, y, layer_dims, X_test=None, y_test=None, lr=0.01, num_iters=1000, \n",
    "                  print_loss=False, add_del=False, reg_param=0.,delta=0.01, prob=1., \n",
    "                  epsilon=1e-3, max_hidden_size=200, tau=50):\n",
    "                  #del_threshold=0.03, prob_del=0.05, prob_add=0.05, max_hidden_size=300, num_below_margin=5):\n",
    "    \n",
    "    parameters, losses, test_losses, num_neurons = \\\n",
    "        MLP(X, y, layer_dims, 'multiclass', X_test, y_test, lr, num_iters, print_loss, add_del, \n",
    "            reg_param, delta,prob,epsilon,max_hidden_size,tau)\n",
    "    return parameters, losses, test_losses, num_neurons\n",
    "\n",
    "def MulticlassStochasticMLP(X, y, layer_dims, X_test=None, y_test=None, optimizer='sgd', \n",
    "                  lr=0.0007, batch_size=64, beta1=0.9, beta2=0.999, eps=1e-8, print_loss=False,\n",
    "                  num_epochs=10000, add_del=False, print_add_del=False, reg_param=0.,\n",
    "                  delta=0.01, prob=0.5, epsilon=0.05, max_hidden_size=100, tau=30):\n",
    "                  #del_threshold=0.03, prob_del=1., prob_add=1., max_hidden_size=300, num_below_margin=1):\n",
    "    \n",
    "    parameters, losses, test_losses = \\\n",
    "        StochasticMLP(X, y, layer_dims, 'multiclass', X_test, y_test, optimizer, lr, batch_size,\n",
    "                  beta1, beta2, eps, num_epochs, print_loss, add_del, reg_param,\n",
    "                  delta,prob,epsilon,max_hidden_size,tau)\n",
    "    \n",
    "    return parameters, losses, test_losses\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    data_size = 7\n",
    "#    num_features = 10\n",
    "#    num_classes = 3\n",
    "#    \n",
    "#    X_train = 10.*np.random.rand(num_features,data_size)\n",
    "#    y_train = np.array([[1,0,0],[0,1,0],[0,0,1],[1,0,0],[0,1,0],[0,0,1],[1,0,0]]).T\n",
    "\n",
    "    mnist = fetch_openml('mnist_784', data_home=os.getcwd())   \n",
    "    #mnist = fetch_openml('MNIST original') \n",
    "    X = mnist.data.astype(np.float32) / 255.\n",
    "    y_orig = mnist.target\n",
    "    # one-hot encode the labels y_orig: i=0,...,9 --> [0,...,1,...,0]\n",
    "    y = pd.get_dummies(y_orig).values.astype(np.float32)\n",
    "    \n",
    "#    pca = PCA(n_components=324)\n",
    "#    pca.fit(X)\n",
    "#    X_pca = pca.transform(X)\n",
    "    \n",
    "    X,y = shuffle(X,y)\n",
    "    \n",
    "    down_sample = 5000\n",
    "    X_ds = X[:down_sample,:]\n",
    "    y_ds = y[:down_sample,:]\n",
    "    \n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_ds,y_ds,test_size=0.2)\n",
    "    X_train = X_train.T\n",
    "    y_train = y_train.T\n",
    "    X_test = X_test.T\n",
    "    y_test = y_test.T\n",
    "    \n",
    "    num_iters = 10000\n",
    "    lr = 0.01\n",
    "    #bs = 128\n",
    "    num_features = X_train.shape[0]\n",
    "    num_classes = y_train.shape[0]\n",
    "    layer_dims = [num_features, 1, num_classes]\n",
    "    parameters,_,ad_loss,num_neurons = \\\n",
    "    MulticlassMLP(X_train, y_train, layer_dims, X_test=X_test,\n",
    "                  y_test=y_test, num_iters=num_iters, add_del=True, \n",
    "                  print_loss=True, lr=lr)\n",
    "    print('training accuracy = %.3f' % score(X_train,y_train,parameters,'multiclass'))\n",
    "    print('test accuracy = %.3f' % score(X_test,y_test,parameters,'multiclass'))\n",
    "#    \n",
    "#    #layer_dims = [num_features, 1, num_classes]\n",
    "#    parameters,_,reg_loss = MulticlassMLP(X_train,y_train, layer_dims, num_iters=num_iters,\n",
    "#                                   X_test=X_test,y_test=y_test,\n",
    "#                                   lr=lr, print_loss=False, add_del=False)     \n",
    "#    print('training accuracy = %.3f' % score(X_train,y_train,parameters,'multiclass'))\n",
    "#    print('test accuracy = %.3f' % score(X_test,y_test,parameters,'multiclass'))\n",
    "    \n",
    "    \n",
    "#    parameters,adam_loss,_ = MulticlassStochasticMLP(X_train, y_train, layer_dims, X_test=None, y_test=None, \n",
    "#                                        num_epochs=num_iters, lr=lr, add_del=False, optimizer='adam', \n",
    "#                                        batch_size=bs, print_loss=False)\n",
    "#    print('train accuracy = %.3f' % score(X_train,y_train,parameters,'multiclass'))\n",
    "#    print('test accuracy = %.3f' % score(X_test,y_test,parameters,'multiclass'))\n",
    "#    parameters,sgd_loss,_ = MulticlassStochasticMLP(X_train, y_train, layer_dims, X_test=None, y_test=None, \n",
    "#                                       num_epochs=num_iters, lr=lr, add_del=False, optimizer='sgd', \n",
    "#                                       batch_size=bs, print_loss=False)\n",
    "#    print('train accuracy = %.3f' % score(X_train,y_train,parameters,'multiclass'))\n",
    "#    print('test accuracy = %.3f' % score(X_test,y_test,parameters,'multiclass'))\n",
    "\n",
    "    ad_loss = np.array(ad_loss)\n",
    "    num_neurons = np.array(num_neurons)\n",
    "\n",
    "    xx = np.linspace(0,num_iters,num=num_iters)+1\n",
    "    plt.plot(xx,.5*np.max(num_neurons)*ad_loss,color='blue',label='val loss')\n",
    "    filt_neurons = lfilter([1.0/50]*50,1,num_neurons)\n",
    "    plt.plot(xx,filt_neurons,color='green',label='neurons')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('iteration')\n",
    "    #plt.ylabel('loss')\n",
    "    plt.title('MNIST')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00dbd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 100879.319113\n",
      "Number of neurons 0: 1\n",
      "Test loss after epoch 0: 101566.993046\n",
      "Loss after iteration 1000: 671.981494\n",
      "Number of neurons 1000: 100\n",
      "Test loss after epoch 1000: 733.320627\n",
      "Loss after iteration 2000: 300.664004\n",
      "Number of neurons 2000: 100\n",
      "Test loss after epoch 2000: 325.206707\n",
      "Loss after iteration 3000: 236.559426\n",
      "Number of neurons 3000: 100\n",
      "Test loss after epoch 3000: 254.137502\n",
      "Loss after iteration 4000: 198.000707\n",
      "Number of neurons 4000: 100\n",
      "Test loss after epoch 4000: 213.514233\n",
      "Loss after iteration 5000: 182.141145\n",
      "Number of neurons 5000: 100\n",
      "Test loss after epoch 5000: 197.093859\n",
      "Loss after iteration 6000: 169.515947\n",
      "Number of neurons 6000: 100\n",
      "Test loss after epoch 6000: 178.722418\n",
      "Loss after iteration 7000: 159.850268\n",
      "Number of neurons 7000: 100\n",
      "Test loss after epoch 7000: 160.813889\n",
      "Loss after iteration 8000: 149.018561\n",
      "Number of neurons 8000: 100\n",
      "Test loss after epoch 8000: 145.101601\n",
      "Loss after iteration 9000: 136.127547\n",
      "Number of neurons 9000: 100\n",
      "Test loss after epoch 9000: 130.333726\n",
      "Loss after iteration 10000: 125.632010\n",
      "Number of neurons 10000: 100\n",
      "Test loss after epoch 10000: 115.249095\n",
      "Loss after iteration 11000: 134.194882\n",
      "Number of neurons 11000: 100\n",
      "Test loss after epoch 11000: 121.035346\n",
      "Loss after iteration 12000: 121.964462\n",
      "Number of neurons 12000: 100\n",
      "Test loss after epoch 12000: 110.640126\n",
      "Loss after iteration 13000: 196.902137\n",
      "Number of neurons 13000: 100\n",
      "Test loss after epoch 13000: 192.760419\n",
      "Loss after iteration 14000: 105.481909\n",
      "Number of neurons 14000: 100\n",
      "Test loss after epoch 14000: 98.621144\n",
      "Loss after iteration 15000: 82.617163\n",
      "Number of neurons 15000: 100\n",
      "Test loss after epoch 15000: 80.025433\n",
      "Loss after iteration 16000: 87.982126\n",
      "Number of neurons 16000: 100\n",
      "Test loss after epoch 16000: 89.828035\n",
      "Loss after iteration 17000: 104.987837\n",
      "Number of neurons 17000: 100\n",
      "Test loss after epoch 17000: 108.775847\n",
      "Loss after iteration 18000: 66.361237\n",
      "Number of neurons 18000: 100\n",
      "Test loss after epoch 18000: 68.792632\n",
      "Loss after iteration 19000: 76.596596\n",
      "Number of neurons 19000: 100\n",
      "Test loss after epoch 19000: 81.193023\n",
      "training R^2 = 0.998\n",
      "test R^2 = 0.998\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Implements a self-selecting multilayer perceptron for scalar regression\n",
    "# Notation used mostly follows Andrew Ng's deeplearning.ai course\n",
    "# Author: Ryan Kingery (rkinger@g.clemson.edu)\n",
    "# Last Updated: April 2018\n",
    "# License: BSD 3 clause\n",
    "\n",
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import lfilter\n",
    "from utils import *\n",
    "np.random.seed(42)\n",
    "\n",
    "def RegressionMLP(X, y, layer_dims, X_test=None, y_test=None, lr=0.01, num_iters=1000, \n",
    "                  print_loss=True, add_del=False, print_add_del=False, \n",
    "                  reg_param=0.,delta=0.01, prob=1., epsilon=0.1, max_hidden_size=100, tau=50):\n",
    "                  #del_threshold=0.03, prob_del=0.05, prob_add=0.05, max_hidden_size=300, num_below_margin=5):\n",
    "    \n",
    "    parameters, losses, test_losses, num_neurons = \\\n",
    "        MLP(X, y, layer_dims, 'regression', X_test, y_test, lr, num_iters, print_loss, \n",
    "            add_del, reg_param, delta,prob,epsilon,max_hidden_size,tau)\n",
    "    return parameters, losses, test_losses, num_neurons\n",
    "\n",
    "def RegressionStochasticMLP(X, y, layer_dims, X_test=None, y_test=None, optimizer='sgd', \n",
    "                  lr=0.0007, batch_size=64, beta1=0.9, beta2=0.999, eps=1e-8, \n",
    "                  num_epochs=10000, print_loss=True,\n",
    "                  add_del=False, print_add_del=False, reg_param=0.,\n",
    "                  delta=0.01, prob=0.5, epsilon=0.05, max_hidden_size=100, tau=30):\n",
    "                  #del_threshold=0.03, prob_del=1., prob_add=1., max_hidden_size=300, num_below_margin=1):\n",
    "    \n",
    "    parameters, losses, test_losses = \\\n",
    "        StochasticMLP(X, y, layer_dims, 'regression', X_test, y_test, optimizer, lr, batch_size,\n",
    "                  beta1, beta2, eps, num_epochs, print_loss, add_del, print_add_del, reg_param,\n",
    "                  delta,prob,epsilon,max_hidden_size,tau)\n",
    "    \n",
    "    return parameters, losses, test_losses\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_size = 1000\n",
    "    num_features = 1\n",
    "    \n",
    "    X = 10.*np.random.rand(num_features,data_size)\n",
    "    #y = 100.*(np.random.choice([1,-1],size=data_size)*np.random.rand(data_size))\n",
    "    y = 10.*X[0,:]**2 - 3.\n",
    "    y = y.reshape(1,-1)\n",
    "    y += 10.*np.random.randn(1,y.shape[1])\n",
    "    y = y.reshape(1,data_size)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.T, y.T, test_size=0.2)\n",
    "    X_train = X_train.T\n",
    "    X_test = X_test.T\n",
    "    y_train = y_train.T\n",
    "    y_test = y_test.T\n",
    "    \n",
    "    num_iters = 20000\n",
    "    lr = 0.1\n",
    "    \n",
    "    layer_dims = [X.shape[0],1, 1]\n",
    "    parameters,_,losses,num_neurons = \\\n",
    "    RegressionMLP(X_train, y_train, layer_dims, num_iters=num_iters,\n",
    "                  X_test=X_test, y_test=y_test,\n",
    "                  lr=0.1, print_loss=True, add_del=True)\n",
    "#    parameters,_,_ = RegressionStochasticMLP(X_train, y_train, layer_dims, optimizer='sgd',\n",
    "#                                             X_test=None,y_test=None,\n",
    "#                                             batch_size=128,lr=0.01,num_epochs=5000, \n",
    "#                                             print_loss=True, add_del=True)\n",
    "    print('training R^2 = %.3f' % score(X_train,y_train,parameters,'regression'))\n",
    "    print('test R^2 = %.3f' % score(X_test,y_test,parameters,'regression'))\n",
    "    \n",
    "    #checking model works\n",
    "    yhat = predict(X,parameters,'regression')\n",
    "    plt.scatter(X[0,:],y[0,:],s=0.2)\n",
    "    plt.scatter(X[0,:],yhat[0,:],color='red',s=0.2)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('$y=10 x^2 - 3$')\n",
    "    plt.show()\n",
    "    \n",
    "    losses = np.array(losses)\n",
    "    num_neurons = np.array(num_neurons)\n",
    "\n",
    "    xx = np.linspace(0,num_iters,num=num_iters)+1\n",
    "    plt.plot(xx,1e-5*np.max(num_neurons)*losses,color='blue',label='val loss')\n",
    "    filt_neurons = lfilter([1.0/50]*50,1,num_neurons)\n",
    "    plt.plot(xx,filt_neurons,color='green',label='neurons')\n",
    "    plt.legend(loc='center right')\n",
    "    plt.xlabel('iteration')\n",
    "    #plt.ylabel('loss')\n",
    "    plt.title('$y=10 x^2 - 3$')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9df7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8c0c566",
   "metadata": {},
   "source": [
    "# Perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6319d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize directory\n",
    "#import ss_perf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef97c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import lfilter\n",
    "import time\n",
    "\n",
    "def delete_neurons_numpy(W1,b1,W2,b2,delta,prob):\n",
    "    \"\"\"\n",
    "    For NumPy models, deletes neurons with small outgoing weights from layer\n",
    "    \n",
    "    Arguments:\n",
    "    W1 -- weight matrix into hidden layer\n",
    "    b1 -- bias vector into hidden layer\n",
    "    W2 -- weight matrix into output layer\n",
    "    b2 -- bias vector into output layer\n",
    "    delta -- threshold for deletion of neurons\n",
    "    prob -- probability of a neuron below threshold being deleted\n",
    "    \n",
    "    Returns:\n",
    "    updated W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    \n",
    "    W_out = W2   \n",
    "    hidden_size = W_out.shape[1]\n",
    "    \n",
    "    norms = np.sum(np.abs(W_out),axis=0)\n",
    "    max_out = np.max(norms)\n",
    "    selected = (norms == norms) # initialize all True == keep all neurons\n",
    "    \n",
    "    for j in range(hidden_size):\n",
    "        norm = norms[j]\n",
    "        if (norm < delta*max_out) and (np.random.rand() < prob):\n",
    "            # remove neuron j with probability prob\n",
    "            selected[j] = 0\n",
    "    \n",
    "    if np.sum(selected) == 0:\n",
    "        # don't want ALL neurons in layer deleted or training will crash\n",
    "        # keep neuron with largest outgoing weights if this occurs\n",
    "        selected[np.argmax(norms)] = 1\n",
    "             \n",
    "    W1 = W1[selected,:]\n",
    "    b1 = b1[selected,:]\n",
    "    W2 = W2[:,selected]\n",
    "        \n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "\n",
    "def add_neurons_numpy(W1,b1,W2,b2,losses,epsilon,delta,max_hidden_size,tau,prob):\n",
    "    \"\"\"\n",
    "    For NumPy models, adds neuron to bottom of layer if loss is stalling\n",
    "    \n",
    "    Arguments:\n",
    "    W1 -- weight matrix into hidden layer\n",
    "    b1 -- bias vector into hidden layer\n",
    "    W2 -- weight matrix into output layer\n",
    "    b2 -- bias vector into output layer\n",
    "    epsilon -- range loss function must deviate to not be flagged as stalling\n",
    "    delta -- threshold for deletion of neurons\n",
    "    max_hidden_size -- max size allowable for the hidden layer\n",
    "    tau -- window size to check for stalling in loss function\n",
    "    prob -- probability of a neuron below threshold being deleted\n",
    "    \n",
    "    Returns:\n",
    "    updated W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    \n",
    "    W_in = W1\n",
    "    b_in = b1\n",
    "    W_out = W2   \n",
    "    hidden_size = W_out.shape[1]\n",
    "    #losses = torch.FloatTensor(losses)\n",
    "    \n",
    "    if hidden_size >= max_hidden_size:\n",
    "        return W1,b1,W2,b2\n",
    "    \n",
    "    max_loss = np.max(losses)\n",
    "    filt_losses = losses#lfilter([1.0/5]*5,1,losses) # filter noise with FIR filter\n",
    "    losses = filt_losses[-tau:]  # keep only losses in window t-tau,...,t\n",
    "    upper = np.mean(losses) + epsilon*max_loss\n",
    "    lower = np.mean(losses) - epsilon*max_loss\n",
    "    num_out_of_window = (losses < lower) + (losses > upper)\n",
    "\n",
    "    if (np.sum(num_out_of_window) == 0) and (np.random.rand() < prob):\n",
    "        # if losses in window are too similar, add neuron with probability prob\n",
    "        ones = np.ones((1,W_in.shape[1]))\n",
    "        new_W_in = np.random.normal(0,2.*delta*ones)\n",
    "        new_b_in = np.zeros((1,1))\n",
    "        ones = np.ones((W_out.shape[0],1))\n",
    "        new_W_out = np.random.normal(0,5.*delta*ones)\n",
    "        W_in = np.append(W_in, new_W_in, axis=0)\n",
    "        b_in = np.append(b_in, new_b_in, axis=0)\n",
    "        W_out = np.append(W_out, new_W_out, axis=1)    \n",
    "    \n",
    "    W1 = W_in\n",
    "    b1 = b_in\n",
    "    W2 = W_out\n",
    "    \n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "def delete_neurons_pytorch(W1,b1,W2,b2,delta,prob):\n",
    "    \"\"\"\n",
    "    For PyTorch models, deletes neurons with small outgoing weights from layer\n",
    "    \n",
    "    Arguments:\n",
    "    W1 -- weight matrix into hidden layer\n",
    "    b1 -- bias vector into hidden layer\n",
    "    W2 -- weight matrix into output layer\n",
    "    b2 -- bias vector into output layer\n",
    "    delta -- threshold for deletion of neurons\n",
    "    prob -- probability of a neuron below threshold being deleted\n",
    "    \n",
    "    Returns:\n",
    "    updated W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    \n",
    "    W_out = W2   \n",
    "    hidden_size = W_out.shape[1]\n",
    "    \n",
    "    norms = torch.sum(torch.abs(W_out),dim=0)\n",
    "    max_out = torch.max(norms)\n",
    "    selected = (norms == norms) # initialize all True == keep all neurons\n",
    "    \n",
    "    for j in range(hidden_size):\n",
    "        norm = norms[j]\n",
    "        if (norm < delta*max_out) and (torch.rand(1) < prob):\n",
    "            # remove neuron j with probability prob\n",
    "            selected[j] = 0\n",
    "    \n",
    "    if torch.sum(selected) == 0:\n",
    "        # don't want ALL neurons in layer deleted or training will crash\n",
    "        # keep neuron with largest outgoing weights if this occurs\n",
    "        selected[torch.argmax(norms)] = 1\n",
    "             \n",
    "    W1 = W1[selected,:]\n",
    "    b1 = b1[selected,:]\n",
    "    W2 = W2[:,selected]\n",
    "        \n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "\n",
    "def add_neurons_pytorch(W1,b1,W2,b2,losses,epsilon,delta,max_hidden_size,tau,\n",
    "                        prob,device):\n",
    "    \"\"\"\n",
    "    For PyTorch models, adds neuron to bottom of layer if loss is stalling\n",
    "    \n",
    "    Arguments:\n",
    "    W1 -- weight matrix into hidden layer\n",
    "    b1 -- bias vector into hidden layer\n",
    "    W2 -- weight matrix into output layer\n",
    "    b2 -- bias vector into output layer\n",
    "    epsilon -- range loss function must deviate to not be flagged as stalling\n",
    "    delta -- threshold for deletion of neurons\n",
    "    max_hidden_size -- max size allowable for the hidden layer\n",
    "    tau -- window size to check for stalling in loss function\n",
    "    prob -- probability of a neuron below threshold being deleted\n",
    "    \n",
    "    Returns:\n",
    "    updated W1, b1, W2, b2\n",
    "    \"\"\"\n",
    "    \n",
    "    W_in = W1\n",
    "    b_in = b1\n",
    "    W_out = W2   \n",
    "    hidden_size = W_out.shape[1]\n",
    "    losses = torch.FloatTensor(losses)\n",
    "    \n",
    "    if hidden_size >= max_hidden_size:\n",
    "        return W1,b1,W2,b2\n",
    "    \n",
    "    max_loss = torch.max(losses)\n",
    "    filt_losses = losses#lfilter([1.0/5]*5,1,losses) # filter noise with FIR filter\n",
    "    losses = filt_losses[-tau:]  # keep only losses in window t-tau,...,t\n",
    "    upper = torch.mean(losses) + epsilon*max_loss\n",
    "    lower = torch.mean(losses) - epsilon*max_loss\n",
    "    num_out_of_window = (losses < lower) + (losses > upper)\n",
    "\n",
    "    if (torch.sum(num_out_of_window).item() == 0) and (torch.rand(1) < prob):\n",
    "        # if losses in window are too similar, add neuron with probability prob\n",
    "        ones = torch.ones(1,W_in.shape[1],device=device)#.cuda()\n",
    "        new_W_in = torch.tensor(torch.normal(0,2.*delta*ones),device=device)#.cuda()\n",
    "        new_b_in = torch.zeros(1,1,device=device)#.cuda()\n",
    "        ones = torch.ones(W_out.shape[0],1,device=device)#.cuda()\n",
    "        new_W_out = torch.tensor(torch.normal(0,5.*delta*ones),device=device)#.cuda()\n",
    "        W_in = torch.cat((W_in, new_W_in), dim=0)\n",
    "        b_in = torch.cat((b_in, new_b_in), dim=0)\n",
    "        W_out = torch.cat((W_out, new_W_out), dim=1)    \n",
    "    \n",
    "    W1 = W_in\n",
    "    b1 = b_in\n",
    "    W2 = W_out\n",
    "    \n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "def gen_data(samples=1000,var=2.):\n",
    "    \"\"\"\n",
    "    Generates sample data from a Gaussian mixture model with centroid variance var \n",
    "    \"\"\"\n",
    "    centers = 5\n",
    "    M = [np.random.multivariate_normal(np.array([1,0]),.8*np.eye(2)) for i in range(centers)] +\\\n",
    "        [np.random.multivariate_normal(np.array([0,1]),.8*np.eye(2)) for i in range(centers)]\n",
    "    \n",
    "    X = np.zeros((samples,2))\n",
    "    y = np.zeros((samples,))\n",
    "    x1 = []\n",
    "    x2 = []    \n",
    "    for j in range(samples):\n",
    "        i = np.random.randint(2*centers)\n",
    "        m = M[i]\n",
    "        X[j,:] = np.random.multivariate_normal(np.array(m),var*np.eye(2)/centers)\n",
    "        if i<centers:\n",
    "            y[j] = 0\n",
    "            x1 += [X[j,:]]\n",
    "        else:\n",
    "            y[j] = 1\n",
    "            x2 += [X[j,:]]\n",
    "    x1 = np.array(x1).reshape(len(x1),2)\n",
    "    x2 = np.array(x2).reshape(len(x2),2)\n",
    "    return X,y,x1,x2\n",
    "\n",
    "def init_add_del():\n",
    "    delta = 0.01\n",
    "    prob = 1.0\n",
    "    epsilon = 1e-5\n",
    "    max_hidden_size = 100\n",
    "    tau = 50\n",
    "    return delta,prob,epsilon,max_hidden_size,tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db6fd19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([2, 1000])\n",
      "y torch.Size([1, 1000])\n",
      "W1 torch.Size([4, 2])\n",
      "b1 torch.Size([4, 1])\n",
      "W2 torch.Size([1, 4])\n",
      "b2 torch.Size([1, 1])\n",
      "Z1 torch.Size([4, 1000])\n",
      "A torch.Size([4, 1000])\n",
      "z2 torch.Size([1, 1000])\n",
      "yhat torch.Size([1, 1000])\n",
      "dyhat torch.Size([1, 1000])\n",
      "dZ2 torch.Size([1, 1000])\n",
      "dW2 torch.Size([1, 4])\n",
      "db2 torch.Size([1, 1])\n",
      "dA torch.Size([4, 1000])\n",
      "dZ1 torch.Size([4, 1000])\n",
      "dW1 torch.Size([4, 2])\n",
      "db1 torch.Size([4, 1])\n",
      "loss after iteration 0: 0.870671\n",
      "# neurons after iteration 0: 4\n",
      "time = 0.053600\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PyTorch implementation of self-selecting MLP for binary classification\n",
    "# Author: Ryan Kingery (rkinger@g.clemson.edu)\n",
    "# Last Updated: May 2018\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import lfilter\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "#from ss_perf_utils import *\n",
    "\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "global device,dtype\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.float\n",
    "\n",
    "def train_pytorch(X,y,layer_dims,num_iters,lr=0.01,add_del=False):\n",
    "    sigmoid = lambda z : 1./(1+torch.exp(-z))\n",
    "    \n",
    "    din,dh,dout = tuple(layer_dims)\n",
    "    m = X.shape[1]\n",
    "    delta,prob,epsilon,max_hidden_size,tau = init_add_del()\n",
    "    losses = []\n",
    "    num_neurons = []\n",
    "    \n",
    "    print ('X',X.shape)\n",
    "    print ('y',y.shape)\n",
    "    \n",
    "    W1 = torch.randn(dh, din, dtype=dtype, requires_grad=False, device=device)\n",
    "    print ('W1',W1.shape)\n",
    "    b1 = torch.randn(dh, 1, dtype=dtype, requires_grad=False, device=device)\n",
    "    print ('b1',b1.shape)\n",
    "    W2 = torch.randn(dout, dh, dtype=dtype, requires_grad=False, device=device)\n",
    "    print ('W2',W2.shape)\n",
    "    b2 = torch.randn(dout, 1, dtype=dtype, requires_grad=False, device=device)\n",
    "    print ('b2',b2.shape)\n",
    "    \n",
    "    for t in range(num_iters):\n",
    "        # Forwardprop\n",
    "        Z1 = torch.mm(W1,X)+b1\n",
    "        print ('Z1',Z1.shape)\n",
    "        A = Z1.clamp(min=0) # relu\n",
    "        print ('A',Z1.shape)\n",
    "        Z2 = torch.mm(W2,A)+b2\n",
    "        print ('z2',Z2.shape)\n",
    "        yhat = sigmoid(Z2).clamp(1e-6,1.-1e-6)\n",
    "        print ('yhat',yhat.shape)\n",
    "    \n",
    "        criterion = nn.BCELoss()\n",
    "        loss = criterion(yhat,y)\n",
    "        loss = loss.squeeze_().item()\n",
    "        losses.append(loss)\n",
    "    \n",
    "        # Backprop\n",
    "        dyhat = -(torch.div(y,yhat) - torch.div(1-y, 1-yhat))\n",
    "        print ('dyhat',dyhat.shape)\n",
    "        dZ2 = dyhat*sigmoid(Z2)*(1-sigmoid(Z2))\n",
    "        print ('dZ2',dZ2.shape)\n",
    "        dW2 = 1./m*torch.mm(dZ2,A.t())\n",
    "        print ('dW2',W2.shape)\n",
    "        db2 = 1./m*torch.sum(dZ2,1,keepdim=True)\n",
    "        print ('db2',db2.shape)\n",
    "        dA = torch.mm(W2.t(),dZ2)\n",
    "        print ('dA',dA.shape)\n",
    "        dZ1 = dA\n",
    "        dZ1[Z1 < 0] = 0\n",
    "        print ('dZ1',dZ1.shape)\n",
    "        dW1 = 1./m*torch.mm(dZ1,X.t())\n",
    "        print ('dW1',dW1.shape)\n",
    "        db1 = 1./m*torch.sum(dZ1,1,keepdim=True)\n",
    "        print ('db1',db1.shape)\n",
    "    \n",
    "        # gradient descent\n",
    "        W1 -= lr*dW1\n",
    "        b1 -= lr*db1\n",
    "        W2 -= lr*dW2\n",
    "        b2 -= lr*db2\n",
    "\n",
    "        if add_del and t>tau:\n",
    "            W1,b1,W2,b2 = delete_neurons_pytorch(W1,b1,W2,b2,delta,prob)\n",
    "            W1,b1,W2,b2 = add_neurons_pytorch(W1,b1,W2,b2,losses,epsilon,delta,\n",
    "                                              max_hidden_size,tau,prob,device)\n",
    "        num_neurons.append(b1.shape[0])\n",
    "\n",
    "        if t % max(1,num_iters // 20) == 0:\n",
    "            print('loss after iteration %i: %f' % (t, losses[-1]))\n",
    "            if add_del:\n",
    "                print('# neurons after iteration %i: %d' % (t, num_neurons[-1]))\n",
    "    \n",
    "    return losses,num_neurons\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_iters = 1\n",
    "    num_samples = 1000\n",
    "    num_features = 2\n",
    "    num_hidden = 4\n",
    "    num_classes = 1\n",
    "    lr = 0.1\n",
    "    layer_dims = [num_features,num_hidden,num_classes]\n",
    "    \n",
    "    X,y,x1,x2 = gen_data(samples=num_samples,var=0.01)\n",
    "    X = torch.tensor(X,device=device,dtype=dtype).t()\n",
    "    y = torch.tensor(y,device=device,dtype=dtype).reshape(1,-1)\n",
    "    \n",
    "    tin = time.clock()\n",
    "    losses,num_neurons = train_pytorch(X,y,layer_dims,num_iters,lr=lr,add_del=True)\n",
    "    tout = time.clock()\n",
    "    tdiff = tout-tin\n",
    "    print('time = %f' % tdiff)\n",
    "    \n",
    "    losses = np.array(losses)\n",
    "    filt_neurons = lfilter([1.0/50]*50,1,num_neurons)\n",
    "    filt_neurons[filt_neurons<1] = num_hidden\n",
    "    \n",
    "    plt.plot(losses,color='blue')\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(filt_neurons,color='green')\n",
    "    plt.title('# Neurons')\n",
    "    plt.show()    \n",
    "    \n",
    "    #plot_model(model,x1,x2)\n",
    "    #print score(model,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8be690a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f45bc71c5b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import lfilter\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "#from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "\n",
    "#from ss_perf_utils import *\n",
    "\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88821330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_numpy(X,y,layer_dims,num_iters,lr,add_del=False):\n",
    "    sigmoid = lambda z : 1./(1+np.exp(-z))\n",
    "    \n",
    "    din,dh,dout = tuple(layer_dims)\n",
    "    m = X.shape[1]\n",
    "    delta,prob,epsilon,max_hidden_size,tau = init_add_del()\n",
    "    losses = []\n",
    "    num_neurons = []\n",
    "    \n",
    "    W1 = np.random.randn(dh,din)\n",
    "    b1 = np.random.randn(dh,1)\n",
    "    W2 = np.random.randn(dout,dh)\n",
    "    b2 = np.random.randn(dout,1)\n",
    "    \n",
    "    for t in range(num_iters):\n",
    "        # Forwardprop\n",
    "        Z1 = np.dot(W1,X)+b1\n",
    "        A = Z1.clip(min=0) # relu\n",
    "        Z2 = np.dot(W2,A)+b2\n",
    "        yhat = sigmoid(Z2).clip(1e-6,1.-1e-6)\n",
    "    \n",
    "        loss = 1./m*(-np.dot(y,np.log(yhat).T)-np.dot(1-y,np.log(1-yhat).T))\n",
    "        loss = loss.squeeze().item()\n",
    "        losses.append(loss)\n",
    "    \n",
    "        # Backprop\n",
    "        dyhat = -(np.divide(y,yhat) - np.divide(1-y, 1-yhat))\n",
    "        dZ2 = dyhat*sigmoid(Z2)*(1-sigmoid(Z2))\n",
    "        dW2 = 1./m*np.dot(dZ2,A.T)\n",
    "        db2 = 1./m*np.sum(dZ2,1,keepdims=True)\n",
    "        dA = np.dot(W2.T,dZ2)\n",
    "        dZ1 = dA\n",
    "        dZ1[Z1 < 0] = 0\n",
    "        dW1 = 1./m*np.dot(dZ1,X.T)\n",
    "        db1 = 1./m*np.sum(dZ1,1,keepdims=True)\n",
    "    \n",
    "        # gradient descent\n",
    "        W1 -= lr*dW1\n",
    "        b1 -= lr*db1\n",
    "        W2 -= lr*dW2\n",
    "        b2 -= lr*db2\n",
    "\n",
    "        if add_del and t>tau:\n",
    "            W1,b1,W2,b2 = delete_neurons_numpy(W1,b1,W2,b2,delta,prob)\n",
    "            W1,b1,W2,b2 = add_neurons_numpy(W1,b1,W2,b2,losses,epsilon,delta,\n",
    "                                              max_hidden_size,tau,prob)\n",
    "        num_neurons.append(b1.shape[0])\n",
    "\n",
    "        if 0==1:#t % max(1,num_iters // 20) == 0:\n",
    "            print('loss after iteration %i: %f' % (t, losses[-1]))\n",
    "            if add_del:\n",
    "                print('# neurons after iteration %i: %d' % (t, num_neurons[-1]))\n",
    "    \n",
    "    return losses,num_neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c034745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch(X,y,layer_dims,num_iters,lr,device,dtype,add_del=False):\n",
    "    sigmoid = lambda z : 1./(1+torch.exp(-z))\n",
    "    \n",
    "    din,dh,dout = tuple(layer_dims)\n",
    "    m = X.shape[1]\n",
    "    delta,prob,epsilon,max_hidden_size,tau = init_add_del()\n",
    "    losses = []\n",
    "    num_neurons = []\n",
    "    \n",
    "    W1 = torch.randn(dh, din, dtype=dtype, requires_grad=False, device=device)\n",
    "    b1 = torch.randn(dh, 1, dtype=dtype, requires_grad=False, device=device)\n",
    "    W2 = torch.randn(dout, dh, dtype=dtype, requires_grad=False, device=device)\n",
    "    b2 = torch.randn(dout, 1, dtype=dtype, requires_grad=False, device=device)\n",
    "    \n",
    "    for t in range(num_iters):\n",
    "        # Forwardprop\n",
    "        Z1 = torch.mm(W1,X)+b1\n",
    "        A = Z1.clamp(min=0) # relu\n",
    "        Z2 = torch.mm(W2,A)+b2\n",
    "        yhat = sigmoid(Z2).clamp(1e-6,1.-1e-6)\n",
    "    \n",
    "        criterion = nn.BCELoss()\n",
    "        loss = criterion(yhat,y)\n",
    "        loss = loss.squeeze_().item()\n",
    "        losses.append(loss)\n",
    "    \n",
    "        # Backprop\n",
    "        dyhat = -(torch.div(y,yhat) - torch.div(1-y, 1-yhat))\n",
    "        dZ2 = dyhat*sigmoid(Z2)*(1-sigmoid(Z2))\n",
    "        dW2 = 1./m*torch.mm(dZ2,A.t())\n",
    "        db2 = 1./m*torch.sum(dZ2,1,keepdim=True)\n",
    "        dA = torch.mm(W2.t(),dZ2)\n",
    "        dZ1 = dA\n",
    "        dZ1[Z1 < 0] = 0\n",
    "        dW1 = 1./m*torch.mm(dZ1,X.t())\n",
    "        db1 = 1./m*torch.sum(dZ1,1,keepdim=True)\n",
    "    \n",
    "        # gradient descent\n",
    "        W1 -= lr*dW1\n",
    "        b1 -= lr*db1\n",
    "        W2 -= lr*dW2\n",
    "        b2 -= lr*db2\n",
    "\n",
    "        if add_del and t>tau:\n",
    "            W1,b1,W2,b2 = delete_neurons_pytorch(W1,b1,W2,b2,delta,prob)\n",
    "            W1,b1,W2,b2 = add_neurons_pytorch(W1,b1,W2,b2,losses,epsilon,delta,max_hidden_size,tau,prob,device)\n",
    "        num_neurons.append(b1.shape[0])\n",
    "\n",
    "        if 0==1:#t % max(1,num_iters // 20) == 0:\n",
    "            print('loss after iteration %i: %f' % (t, losses[-1]))\n",
    "            if add_del:\n",
    "                print('# neurons after iteration %i: %d' % (t, num_neurons[-1]))\n",
    "    \n",
    "    return losses,num_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70366a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tensorflow(X,y,layer_dims,num_iters,lr):\n",
    "    din,dh,dout = tuple(layer_dims)\n",
    "    m = X.shape[0]\n",
    "    delta,prob,epsilon,max_hidden_size,tau = init_add_del()\n",
    "    losses = []\n",
    "    num_neurons = []\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(dh, activation='relu', input_dim=din))\n",
    "    model.add(Dense(dout, activation='sigmoid'))\n",
    "    sgd = SGD(lr=lr, decay=1., momentum=0., nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=sgd)\n",
    "    model.fit(X, y, epochs=num_iters, batch_size=y.shape[0], verbose=0)\n",
    "    \n",
    "    return losses,num_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77fad63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 100\n",
    "num_iters = 1000\n",
    "num_samples = 10000\n",
    "num_features = 2\n",
    "num_hidden = 10\n",
    "num_classes = 1\n",
    "lr = 0.1\n",
    "layer_dims = [num_features,num_hidden,num_classes]\n",
    "\n",
    "X,y,x1,x2 = gen_data(samples=num_samples,var=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bdb08ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [13:13<00:00,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_np = X.T\n",
    "y_np = y.reshape(1,-1)\n",
    "\n",
    "times_np = []\n",
    "print('numpy starting')\n",
    "for run in tqdm(range(num_runs)):\n",
    "    tin = time.clock()\n",
    "    losses,num_neurons = train_numpy(X_np,y_np,layer_dims,num_iters,lr,add_del=False)\n",
    "    tout = time.clock()\n",
    "    times_np.append(tout-tin)\n",
    "print('numpy finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9dcd1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch cpu starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [02:43<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch cpu finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "dtype = torch.float\n",
    "\n",
    "X_pt = torch.tensor(X,device=device,dtype=dtype).t()\n",
    "y_pt = torch.tensor(y,device=device,dtype=dtype).reshape(1,-1)\n",
    "\n",
    "\n",
    "times_pt_cpu = []\n",
    "print('pytorch cpu starting')\n",
    "for run in tqdm(range(num_runs)):\n",
    "    tin = time.clock()\n",
    "    losses,num_neurons = train_pytorch(X_pt,y_pt,layer_dims,num_iters,lr,device,dtype,add_del=False)\n",
    "    tout = time.clock()\n",
    "    times_pt_cpu.append(tout-tin)\n",
    "print('pytorch cpu finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "dtype = torch.float\n",
    "\n",
    "X_pt = torch.tensor(X,device=device,dtype=dtype).t()\n",
    "y_pt = torch.tensor(y,device=device,dtype=dtype).reshape(1,-1)\n",
    "\n",
    "\n",
    "times_pt_gpu = []\n",
    "print('pytorch gpu starting')\n",
    "for run in tqdm(range(num_runs)):\n",
    "    tin = time.clock()\n",
    "    losses,num_neurons = train_pytorch(X_pt,y_pt,layer_dims,num_iters,lr,device,dtype,add_del=False)\n",
    "    tout = time.clock()\n",
    "    times_pt_gpu.append(tout-tin)\n",
    "print('pytorch gpu finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f9a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "X_tf = X\n",
    "y_tf = y\n",
    "\n",
    "times_tf = []\n",
    "print('tensorflow starting')\n",
    "for run in tqdm(range(num_runs)):\n",
    "    tin = time.clock()\n",
    "    losses,num_neurons = train_tensorflow(X_tf,y_tf,layer_dims,num_iters,lr)\n",
    "    tout = time.clock()\n",
    "    times_tf.append(tout-tin)\n",
    "print('tensorflow finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acedfd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(times_np)\n",
    "plt.title('numpy times')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a73d59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAASrUlEQVR4nO3de5BkZXnH8e9PFq+rXGQKQcFVQQ0as+hKtDTGa0RRUUNSUBHRoKuJKEaTiGipsTSFJmK0MOoqBLwBhkvEeImIWATL26LIVQVhEQHZJYiA912e/NFnsB1npnsuPT3v8v1Ude3pc94+53l2Z39z5u3TZ1JVSJLac6dxFyBJmh8DXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4thpJNiR56rjrGCTJrUkeOO461D4DXEsmyaoklWTFuGtZKkm+nOQl/euqamVVXTGumrT1MMDVjDtS8EvDMMA1Z91UxeuTXJLkJ0n+I8ldu20XJXl239htk9yQZG/gnG71Td00wmOT3CnJG5NclWRjko8k2a577eQZ+6FJfgh8qVv/0iSXJrmlq+GRfeWtTnJBkp8mOXmyrhn6mHY/A/p7UZJzp+ynkuwxzf7fDvwJcEzX7zFTxyc5Psm/J/lcN+YrSe6T5N+6Y3+3+7ub3OeuSU5NsinJlUle1bdtnyTrk9yc5PokRw/+11TTqsqHjzk9gA3ARcBuwI7AV4C3ddv+ETi5b+z+wIXd8iqggBV92/8auBx4ILASOA346JTxHwHuAdwN+AvgGuDRQIA9gPv31fUNYNeurkuBl8/Qw6D9zNTfi4Bzp+yrgD1mOM6XgZfMNB44HrgBeBRwV3rfpK4EXghsA7wNOLsbeyfgPOBNwJ27v7MrgKd3278KHNwtrwQeM+6vFR+jfXgGrvk6pqqurqobgbcDB3XrPwY8M8m9uucHAx+dZT9/BRxdVVdU1a3A64EDp0yXvKWqflZVvwBeAryzqr5ZPZdX1VV9Y99bVdd2dX0aWD3DcQftZ6b+RuH0qjqvqn4JnA78sqo+UlVbgJOByTPwRwMTVfXWqvp19ebRPwQc2G3/DbBHkp2q6taq+toIa9YyYIBrvq7uW76K3lkvVXUtvTPWP0+yPfAM4OOz7GfX7vX9+1oB7DzDsXYDfjDL/n7ct/xzemei0xm0n2n7G5Hr+5Z/Mc3zyR7uD+ya5KbJB3Akv/27OhR4MPDdJN9M8qwR1qxlwDeFNF+79S3vDlzb9/wEeme4K4CvVtU13frpbn15Lb1g6t/XZnohdr9pXnc18KD5lz30fmbq72fA3Sc3JLnPgOMs5u0+rwaurKo9pz1Q1WXAQUnuBDwfOCXJvavqZ4tYg5YRz8A1X69Icr8kOwJvoPej/qT/Ah4JHE5v/nrSJuA2enO3k04E/i7JA5KsBP6Z3hz65hmO+2Hg75M8Kj17JLn/DGNnM2g/M/X3HeBhSVZ3b2y+ZcBxrud3+12IbwC3JHldkrsl2SbJw5M8GiDJC5JMVNVtwE3da25bpGNrGTLANV+fAL5A7020H9B7sw2Abq76VOAB9N6UnFz/c3rzyV/ppgAeAxxHb478HHpv3v0SeOVMB62q/+z28QngFnrfLHaca/FD7Gfa/qrq+8BbgS8ClwG/c0XKNN4DHNBdUfLeudY5peYtwLPozetfSe/Nzw8D23VD9gUuTnJrd9wDu38LbaVS5S900Nwk2UDvyoovzjLmTcCDq+oFS1bYIhmmP2k5cA5ci66bdjiU3hUokkbEKRQtqiQvpfdm2+eq6pxB4yXNn1MoktQoz8AlqVED58C7S6XOAe7SjT+lqt6c5AHAScC96X289+Cq+vVs+9ppp51q1apVCy5aku5IzjvvvBuqamLq+mHexPwV8OSqujXJtsC5ST4HvAZ4d1WdlOQD9N60ev9sO1q1ahXr16+fR/mSdMeV5Krp1g+cQunuE3Fr93Tb7lHAk4FTuvUnAM9deJmSpGENNQfefeLrfGAjcCa9Dzbc1PdpuR8B9x1JhZKkaQ0V4FW1papW07s3xT7AQ4c9QJK13T2K12/atGl+VUqSfs+crkKpqpuAs4HHAtv33fLzfvTurTzda9ZV1ZqqWjMx8Xtz8JKkeRoY4EkmutuCkuRuwNPo3Sj/bOCAbtghwKdGVKMkaRrDXIWyC3BCkm3oBf4nq+q/k1wCnJTkbcC3gWNHWKckaYqBAV5VF/Db3wjSv/4KevPhkqQx8JOYktQoA1ySGuXtZJexVUd8ZizH3XDUfmM5rqS58QxckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmN8pca6/eM65cpg79QWZoLz8AlqVEGuCQ1ygCXpEYNDPAkuyU5O8klSS5Ocni3/i1Jrklyfvd45ujLlSRNGuZNzM3Aa6vqW0nuCZyX5Mxu27ur6l9HV54kaSYDA7yqrgOu65ZvSXIpcN9RFyZJmt2c5sCTrAL2Br7erTosyQVJjkuywwyvWZtkfZL1mzZtWli1kqTbDR3gSVYCpwKvrqqbgfcDDwJW0ztDf9d0r6uqdVW1pqrWTExMLLxiSRIwZIAn2ZZeeH+8qk4DqKrrq2pLVd0GfAjYZ3RlSpKmGuYqlADHApdW1dF963fpG/Y84KLFL0+SNJNhrkJ5HHAwcGGS87t1RwIHJVkNFLABeNkI6pMkzWCYq1DOBTLNps8ufjmSpGH5SUxJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEDAzzJbknOTnJJkouTHN6t3zHJmUku6/7cYfTlSpImDXMGvhl4bVXtBTwGeEWSvYAjgLOqak/grO65JGmJDAzwqrquqr7VLd8CXArcF9gfOKEbdgLw3BHVKEmaxpzmwJOsAvYGvg7sXFXXdZt+DOw8w2vWJlmfZP2mTZsWUqskqc/QAZ5kJXAq8Oqqurl/W1UVUNO9rqrWVdWaqlozMTGxoGIlSb81VIAn2ZZeeH+8qk7rVl+fZJdu+y7AxtGUKEmazjBXoQQ4Fri0qo7u23QGcEi3fAjwqcUvT5I0kxVDjHkccDBwYZLzu3VHAkcBn0xyKHAV8JcjqVCSNK2BAV5V5wKZYfNTFrccSdKw/CSmJDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY1aMe4CpH6rjvjMWI674aj9xnJcaSE8A5ekRhngktQoA1ySGjUwwJMcl2Rjkov61r0lyTVJzu8ezxxtmZKkqYY5Az8e2Hea9e+uqtXd47OLW5YkaZCBAV5V5wA3LkEtkqQ5WMhlhIcleSGwHnhtVf1kukFJ1gJrAXbfffcFHG58xnVpmyTNZr5vYr4feBCwGrgOeNdMA6tqXVWtqao1ExMT8zycJGmqeQV4VV1fVVuq6jbgQ8A+i1uWJGmQeQV4kl36nj4PuGimsZKk0Rg4B57kROCJwE5JfgS8GXhiktVAARuAl42uREnSdAYGeFUdNM3qY0dQiyRpDvwkpiQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYNDPAkxyXZmOSivnU7JjkzyWXdnzuMtkxJ0lTDnIEfD+w7Zd0RwFlVtSdwVvdckrSEBgZ4VZ0D3Dhl9f7ACd3yCcBzF7csSdIg850D37mqruuWfwzsPNPAJGuTrE+yftOmTfM8nCRpqgW/iVlVBdQs29dV1ZqqWjMxMbHQw0mSOvMN8OuT7ALQ/blx8UqSJA1jvgF+BnBIt3wI8KnFKUeSNKxhLiM8Efgq8JAkP0pyKHAU8LQklwFP7Z5LkpbQikEDquqgGTY9ZZFrkSTNgZ/ElKRGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIatWIhL06yAbgF2AJsrqo1i1GUJGmwBQV450lVdcMi7EeSNAdOoUhSoxZ6Bl7AF5IU8MGqWjd1QJK1wFqA3XfffYGHk0Zj1RGfGduxNxy139iOrbYt9Az88VX1SOAZwCuSPGHqgKpaV1VrqmrNxMTEAg8nSZq0oACvqmu6PzcCpwP7LEZRkqTB5h3gSe6R5J6Ty8CfARctVmGSpNktZA58Z+D0JJP7+URVfX5RqpIkDTTvAK+qK4A/WsRaJElz4GWEktSoxfggz5IY52VekrQceQYuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhrVzO1kpa3VHfFWyRuO2m/cJWwVPAOXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjfIyQkl3GOO8ZHMUl056Bi5JjTLAJalRBrgkNWpBAZ5k3yTfS3J5kiMWqyhJ0mDzDvAk2wDvA54B7AUclGSvxSpMkjS7hZyB7wNcXlVXVNWvgZOA/RenLEnSIAu5jPC+wNV9z38E/PHUQUnWAmu7p7cm+d6A/e4E3LCAupaTraUX+1hemu8j77h9sfleOgP76Ot5Pu4/3cqRXwdeVeuAdcOOT7K+qtaMsKQls7X0Yh/Ly9bSB2w9vYyrj4VMoVwD7Nb3/H7dOknSElhIgH8T2DPJA5LcGTgQOGNxypIkDTLvKZSq2pzkMOB/gG2A46rq4kWoaejplgZsLb3Yx/KytfQBW08vY+kjVTWO40qSFshPYkpSowxwSWrUWAI8yW5Jzk5ySZKLkxw+zZh/SHJ+97goyZYkO46j3pkM2cd2ST6d5DvdmBePo9ZBhuxlhySnJ7kgyTeSPHwctc4myV272ib/vv9pmjF3SXJydwuIrydZNYZSZzVkH09I8q0km5McMI46Bxmyj9d0X3cXJDkrybTXPI/bkL28PMmFXW6dO/JPp1fVkj+AXYBHdsv3BL4P7DXL+GcDXxpHrQvtAzgSeEe3PAHcCNx53LXPs5d/Ad7cLT8UOGvcdU/TR4CV3fK2wNeBx0wZ87fAB7rlA4GTx133PPtYBTwC+AhwwLhrXkAfTwLu3i3/zXL895hDL/fqW34O8PlR1jSWM/Cquq6qvtUt3wJcSu+TnTM5CDhxKWqbiyH7KOCeSQKspBfgm5e00CEM2ctewJe6Md8FViXZeUkLHaB6bu2ebts9pr5Tvz9wQrd8CvCU7t9n2Rimj6raUFUXALctdX3DGrKPs6vq593Tr9H7TMmyM2QvN/c9vcfU7Ytt7HPg3Y+ve9P7bjbd9rsD+wKnLmFZczZLH8cAfwBcC1wIHF5Vy/Y/HMzay3eA53dj9qH38d5l958tyTZJzgc2AmdW1dQ+br8NRFVtBn4K3HtJixzCEH00YY59HAp8bkkKm4dheknyiiQ/AN4JvGqU9Yw1wJOspBfMr57ynavfs4GvVNWNS1fZ3Azo4+nA+cCuwGrgmCT3WtIC52BAL0cB23dfwK8Evg1sWdoKB6uqLVW1mt43l32W41z9MO5ofSR5AbCG3lTdsjRML1X1vqp6EPA64I2jrGdsAZ5kW3pB8fGqOm2WoQeyDKdPJg3Rx4uB07ofvy4HrqQ3f7zsDOqlqm6uqhd3X8AvpDenf8XSVjm8qroJOJveT3D9br8NRJIVwHbA/y1pcXMwSx9Nma2PJE8F3gA8p6p+tcSlzdmQ/yYnAc8dZR3jugolwLHApVV19CzjtgP+FPjUUtU2F0P28UPgKd34nYGHsAxDb5hekmzf3TYB4CXAObP85DQWSSaSbN8t3w14GvDdKcPOAA7plg+g9wb5svpE25B9LHvD9JFkb+CD9MJ745IXOaQhe9mz7+l+wGUjrWkcX7dJHg/8L7054cn54COB3QGq6gPduBcB+1bVgUte5BCG6SPJrsDx9K7yCHBUVX1s6aud3ZC9PJbem38FXAwcWlU/GUO5M0ryCHo1bkPvBOWTVfXWJG8F1lfVGUnuCnyU3jz/jcCBVbWsvqkO2cejgdOBHYBfAj+uqoeNrehpDNnHF4E/BK7rXvbDqnrOeCqe2ZC9vAd4KvAb4CfAYbU4txiZvqZlduIhSRrS2K9CkSTNjwEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGvX/JryZbmnNtsUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(times_pt_cpu)\n",
    "plt.title('pytorch cpu times')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b58c15f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'times_pt_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-ea8b0e535b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_pt_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pytorch gpu times'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'times_pt_gpu' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(times_pt_gpu)\n",
    "plt.title('pytorch gpu times')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65ce94e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEICAYAAACpqsStAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAASu0lEQVR4nO3de7SldX3f8fcnMyCIIrcRgQFmEvEytanUKSHBBArejUITYqE1GQ2GpKu52qho0orNasS2K2iXtq4RY1kYb6VGSWJrKAKNl44ZBFRuMpmAM4DMiIwoNRXw2z+e35DN8Zw5e+acfc7+xfdrrb3Oc93PZz/n4XOe/TyzN6kqJEl9+qHlDiBJ2neWuCR1zBKXpI5Z4pLUMUtckjpmiUtSxyxxdSGD9yW5P8nnk5yWZPty55qEJG9Kcsly51AfLHE9RpI7kjxvuXPM4rnA84HVVXXScodZLLP9Maqq36+q1yxXJvXFEtfUS7ISOB64o6oeXO480jSxxPWoJJcBxwF/kuTbSV7fpp+c5LNJdiW5MclpI+tck+T3knwmybeS/HmSI9q8A5K8P8l9bd2/THJkm3d0kiuSfCPJliS/NPKcFya5vK37AHAecAnw4y3XW2bJ/syWZVeSm5K8vE1f26b9UBt/T5Ido685yW/OsT+OTfLRJDvba3hnm/6q9nrfmeSbSW5NcsbIeo95N9Nez/tnef6DgP8BHN1e17fbfnl0+SRrklSSVyfZ1i4n/UqSf5Tki+21vXPG8/5iklvasp9McnybniQXJ9mR5IEkX0ryrNleuzpSVT58PPoA7gCeNzJ+DHAf8BKGP/rPb+Or2vxrgL8CngYc2MYvavN+GfgT4PHACuA5wMFt3v8G/jNwAPBsYCdwept3IfAQcFbb5oHAq4BPj+Q6DdjehvcDtgBvAvYHTge+BTy9zf8q8Jw2fBuwFXjmyLwTZ9kPK4AbgYuBg1rO57Z5rwIeBn6rbfufAt8EDptjH14IvH+O/f3o65hteWANUMC7W4YXAH8DfAx4cvv97ABObcuf2fbFM4GVwO8Cn23zXghcBxwCpC1z1HIfcz4W9vBMXPN5JfCJqvpEVX2vqq4ENjOU+m7vq6qvVNV3gI8wlDIMRXw48NSqeqSqrquqB5IcC5wCvKGq/qaqbmA40/6Fkef8XFV9rG3zO/NkPBl4AsMfj+9W1aeAPwXObfOvBU5N8pQ2fnkbXwsczFDWM50EHA28rqoebDk/PTJ/B/D2qnqoqj7M8MfhpfPkXIjfaxn+HHgQ+GBV7aiqu4C/AE5sy/0K8NaquqWqHgZ+H3h2Oxt/CHgi8AwgbZl7JphZS8AS13yOB36uvW3flWQXw03Go0aW+drI8P9lKFSAy4BPAh9KcneSf59kP4Zy/EZVfWtkvTsZzip327YXGY8GtlXV9+Z4vmsZznh/iuEdwDXAqe3xFzPW2+1Y4M5WhLO5q6pGvz3uzpZjUu4dGf7OLOO79/nxwDtGflffYDjrPqb9cXsn8C5gR5KNSQ6eYGYtAUtcM838WsttwGVVdcjI46CqumjeJxrOUt9SVeuAnwB+muFs+27gsCRPHFn8OOCuPeTYk7uBY3df957l+a4FfpKhyK8FPs3wTuDUNj6bbcBx7abqbI5Jkhnbu7sNP8hwCWm3pzC3xf4a0W3AL8/4fR1YVZ8FqKr/VFXPAdYxXAJ73SJvX0vMEtdM9wI/PDL+fuBlSV6YZEW7WXlaktXzPVGSf5zk7ydZATzA8Hb+e1W1Dfgs8Nb2fD/KcPPy+27+jWkTwzuA1yfZr914fRnwIYCqup3hbPWVwLVV9UB7nT/L3CX+eeAe4KIkB7Wcp4zMfzLw6217P8dwffkTbd4NwDlt3nrg7D1kvxc4PMmT9vI1z+XdwBuT/D2AJE9q+Wg3Q3+svRt6kOHa+mzvQtQRS1wzvRX43fZ2/Ldb4Z7JcNNwJ8OZ3usY79h5CsP15weAWxgK87I271yGm3Z3A38MvLmq/te+BK6q7zKU9ouBrzPcMP2Fqrp1ZLFrgfva69k9HuALczznI+05n8pw83M7ww3M3TYBJ7Tt/Tvg7Kq6r83718CPAPcDbwE+sIfstwIfBLa2fb6gSzJV9cfA2xguYT0AfJlhv8Bw/f89LdedDDeo/8NCtqfll8de1pM0nySvAl5TVc9d7iySZ+KS1DFLXJI65uUUSeqYZ+KS1LG5/g3sRBxxxBG1Zs2apdykJHXvuuuu+3pVrZpt3pKW+Jo1a9i8efNSblKSupfkzrnmeTlFkjpmiUtSxyxxSeqYJS5JHbPEJaljlrgkdcwSl6SOWeKS1DFLXJI6tqSf2FyINRf82bJs946LJvn/vpWkhfFMXJI6ZolLUscscUnqmCUuSR2zxCWpY5a4JHXMEpekjlniktQxS1ySOmaJS1LHLHFJ6pglLkkds8QlqWOWuCR1zBKXpI5Z4pLUsbFKPMlvJbkpyZeTfDDJAUnWJtmUZEuSDyfZf9JhJUmPNW+JJzkG+HVgfVU9C1gBnAO8Dbi4qp4K3A+cN8mgkqTvN+7llJXAgUlWAo8H7gFOBy5v8y8Fzlr0dJKkPZq3xKvqLuA/Al9lKO9vAtcBu6rq4bbYduCYSYWUJM1unMsphwJnAmuBo4GDgBeNu4Ek5yfZnGTzzp079zmoJOn7jXM55XnAX1fVzqp6CPgocApwSLu8ArAauGu2latqY1Wtr6r1q1atWpTQkqTBOCX+VeDkJI9PEuAM4GbgauDstswG4OOTiShJmss418Q3MdzA/ALwpbbORuANwGuTbAEOB947wZySpFmsnH8RqKo3A2+eMXkrcNKiJ5Ikjc1PbEpSxyxxSeqYJS5JHbPEJaljlrgkdcwSl6SOWeKS1DFLXJI6ZolLUscscUnqmCUuSR2zxCWpY5a4JHXMEpekjlniktQxS1ySOmaJS1LHLHFJ6pglLkkds8QlqWOWuCR1zBKXpI5Z4pLUMUtckjpmiUtSxyxxSeqYJS5JHbPEJaljlrgkdcwSl6SOWeKS1DFLXJI6ZolLUscscUnqmCUuSR2zxCWpY5a4JHXMEpekjo1V4kkOSXJ5kluT3JLkx5McluTKJLe3n4dOOqwk6bHGPRN/B/A/q+oZwD8AbgEuAK6qqhOAq9q4JGkJzVviSZ4E/BTwXoCq+m5V7QLOBC5ti10KnDWZiJKkuYxzJr4W2Am8L8n1SS5JchBwZFXd05b5GnDkbCsnOT/J5iSbd+7cuTipJUnAeCW+EviHwH+pqhOBB5lx6aSqCqjZVq6qjVW1vqrWr1q1aqF5JUkjxinx7cD2qtrUxi9nKPV7kxwF0H7umExESdJc5i3xqvoasC3J09ukM4CbgSuADW3aBuDjE0koSZrTyjGX+zXgj5LsD2wFXs3wB+AjSc4D7gReMZmIkqS5jFXiVXUDsH6WWWcsahpJ0l7xE5uS1DFLXJI6ZolLUscscUnqmCUuSR2zxCWpY5a4JHXMEpekjlniktQxS1ySOmaJS1LHLHFJ6pglLkkds8QlqWOWuCR1zBKXpI5Z4pLUMUtckjpmiUtSxyxxSeqYJS5JHbPEJaljlrgkdcwSl6SOWeKS1DFLXJI6ZolLUscscUnqmCUuSR2zxCWpY5a4JHXMEpekjlniktQxS1ySOmaJS1LHLHFJ6pglLkkds8QlqWNjl3iSFUmuT/KnbXxtkk1JtiT5cJL9JxdTkjSbvTkT/w3glpHxtwEXV9VTgfuB8xYzmCRpfmOVeJLVwEuBS9p4gNOBy9silwJnTSCfJGkPxj0TfzvweuB7bfxwYFdVPdzGtwPHzLZikvOTbE6yeefOnQvJKkmaYd4ST/LTwI6qum5fNlBVG6tqfVWtX7Vq1b48hSRpDivHWOYU4OVJXgIcABwMvAM4JMnKdja+GrhrcjElSbOZ90y8qt5YVaurag1wDvCpqvrnwNXA2W2xDcDHJ5ZSkjSrhfw78TcAr02yheEa+XsXJ5IkaVzjXE55VFVdA1zThrcCJy1+JEnSuPzEpiR1zBKXpI5Z4pLUMUtckjpmiUtSxyxxSeqYJS5JHbPEJaljlrgkdcwSl6SOWeKS1DFLXJI6ZolLUscscUnqmCUuSR2zxCWpY5a4JHXMEpekjlniktQxS1ySOmaJS1LHLHFJ6pglLkkds8QlqWOWuCR1zBKXpI5Z4pLUMUtckjpmiUtSxyxxSeqYJS5JHbPEJaljlrgkdcwSl6SOWeKS1DFLXJI6ZolLUsfmLfEkxya5OsnNSW5K8htt+mFJrkxye/t56OTjSpJGjXMm/jDwr6pqHXAy8C+TrAMuAK6qqhOAq9q4JGkJzVviVXVPVX2hDX8LuAU4BjgTuLQtdilw1oQySpLmsFfXxJOsAU4ENgFHVtU9bdbXgCPnWOf8JJuTbN65c+dCskqSZhi7xJM8AfjvwG9W1QOj86qqgJptvaraWFXrq2r9qlWrFhRWkvRYY5V4kv0YCvyPquqjbfK9SY5q848CdkwmoiRpLuP865QA7wVuqao/GJl1BbChDW8APr748SRJe7JyjGVOAX4e+FKSG9q0NwEXAR9Jch5wJ/CKiSSUJM1p3hKvqk8DmWP2GYsbR5K0N/zEpiR1zBKXpI5Z4pLUMUtckjpmiUtSxyxxSeqYJS5JHbPEJaljlrgkdcwSl6SOWeKS1DFLXJI6ZolLUscscUnqmCUuSR2zxCWpY5a4JHXMEpekjlniktQxS1ySOmaJS1LHLHFJ6pglLkkds8QlqWOWuCR1zBKXpI5Z4pLUMUtckjpmiUtSxyxxSeqYJS5JHVu53AGm3ZoL/mzZtn3HRS9dtm1L6oNn4pLUMUtckjrm5RTpB5SXCv9u8ExckjrmmbiEZ6U/KJbr9zzJ37Fn4pLUsQWVeJIXJbktyZYkFyxWKEnSePa5xJOsAN4FvBhYB5ybZN1iBZMkzW8hZ+InAVuqamtVfRf4EHDm4sSSJI0jVbVvKyZnAy+qqte08Z8HfqyqfnXGcucD57fRpwO37XvcOR0BfH0CzzsJZp2cnvKadTL+rmY9vqpWzTZj4v86pao2AhsnuY0km6tq/SS3sVjMOjk95TXrZPwgZl3I5ZS7gGNHxle3aZKkJbKQEv9L4IQka5PsD5wDXLE4sSRJ49jnyylV9XCSXwU+CawA/rCqblq0ZHtnopdrFplZJ6envGadjB+4rPt8Y1OStPz8xKYkdcwSl6SOTXWJJzkgyeeT3JjkpiRvmWO5VyS5uS3zgZHpG5Lc3h4bpjzrI0luaI+J3iAeJ2uSi0fyfCXJrpF5U7Vf58m6ZPt1L/Iel+TqJNcn+WKSl4zMe2P7GovbkrxwWrMmWZPkOyP79t1TkPX4JFe1nNckWT0yb9qO2T1l3btjtqqm9gEEeEIb3g/YBJw8Y5kTgOuBQ9v4k9vPw4Ct7eehbfjQaczahr89Tft1xvK/xnDjeir361xZl3q/7sVxsBH4F214HXDHyPCNwOOAtcBfASumNOsa4MtTtl//G7ChDZ8OXDatx+xcWfflmJ3qM/EafLuN7tceM+/E/hLwrqq6v62zo01/IXBlVX2jzbsSeNGUZl1SY2YddS7wwTY8jft1rqxLbsy8BRzchp8E3N2GzwQ+VFX/r6r+GtjC8PUW05h1SY2ZdR3wqTZ8NX/7NSDTeMzOlXWvTXWJw/BFW0luAHYw/CI2zVjkacDTknwmyf9JsvuXcwywbWS57W3aNGYFOCDJ5jb9rEnmHDPr7uWOZzgr3H3ATeN+3b3czKywxPu15Zgv74XAK5NsBz7B8O4BpnPfzpUVYG27zHJtkp+cZM4xs94I/Ewb/ifAE5McznTu17mywl4es1Nf4lX1SFU9m+EToScledaMRVYyXKY4jeEs7D1JDlnKjLstMOvxNXwE958Bb0/yI8ucdbdzgMur6pFJ5tmTBWZd0v0KY+U9F/ivVbUaeAlwWZJl+W9xAVnvAY6rqhOB1wIfSHIwEzRG1t8GTk1yPXAqwyfIl+W4XWDWvTpmp77Ed6uqXQxvO2a+DdoOXFFVD7W3oF9hKMpl+1qAfchKVd3Vfm4FrgFOXOasu53DYy9PTON+3W1m1mXbr22bu5g973nAR9oynwMOYPgypGnct7NmbZd87mvTr2O4fv+05cxaVXdX1c+0Pyy/M7Ls1O3XPWTd62N2qks8yardZ6pJDgSeD9w6Y7GPMZzZkuQIhgNpK8MnSV+Q5NAkhwIvaNOmLmvL+LiR6acANy9zVpI8g+FG0OdGJk/jfp0161Lv173I+1XgjLbMMxmKcSfD11ack+RxSdYy/IH//DRmbeuuaNN/uGXdupxZkxwx8o7mjcAftuGpO2bnyrpPx+y4d0CX4wH8KMO/5vgi8GXg37Tp/xZ4ef3tneA/aC/0S8A5I+v/IsPNoS3Aq6c1K/ATbfzG9vO85c7axi8ELppl/anar3NlXer9uhfHwTrgMy3XDcALRtb/HYaz2tuAF09rVuBngZvatC8AL5uCrGcDtzO8w70EeNy0HrNzZd2XY9aP3UtSx6b6cookac8scUnqmCUuSR2zxCWpY5a4JHXMEpekjlniktSx/w+TwZ/lB0ZCSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(times_tf)\n",
    "plt.title('tensorflow cpu times')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29fe5fd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'times_pt_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2b93aabcede0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'np'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_pt_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_pt_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt gpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'times_pt_gpu' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 70, 100)\n",
    "\n",
    "plt.hist(times_np, bins, alpha=0.5, label='np')\n",
    "plt.hist(times_pt_cpu, bins, alpha=0.5, label='pt cpu')\n",
    "plt.hist(times_pt_gpu, bins, alpha=0.5, label='pt gpu')\n",
    "plt.hist(times_tf, bins, alpha=0.5, label='tf cpu')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('runtime (sec)')\n",
    "plt.title('runtime histograms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "317b94bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.283075869999998\n",
      "2.908265020000026\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'times_pt_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e383a44ef6e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_pt_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_pt_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimes_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'times_pt_gpu' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.mean(times_np))\n",
    "print(np.mean(times_pt_cpu))\n",
    "print(np.mean(times_pt_gpu))\n",
    "print(np.mean(times_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa7fd177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after iteration 0: 1.351279\n",
      "# neurons after iteration 0: 1\n",
      "loss after iteration 500: 0.455735\n",
      "# neurons after iteration 500: 2\n",
      "loss after iteration 1000: 0.325371\n",
      "# neurons after iteration 1000: 2\n",
      "loss after iteration 1500: 0.299435\n",
      "# neurons after iteration 1500: 2\n",
      "loss after iteration 2000: 0.293333\n",
      "# neurons after iteration 2000: 2\n",
      "loss after iteration 2500: 0.290836\n",
      "# neurons after iteration 2500: 2\n",
      "loss after iteration 3000: 0.289461\n",
      "# neurons after iteration 3000: 2\n",
      "loss after iteration 3500: 0.288587\n",
      "# neurons after iteration 3500: 2\n",
      "loss after iteration 4000: 0.287966\n",
      "# neurons after iteration 4000: 2\n",
      "loss after iteration 4500: 0.287518\n",
      "# neurons after iteration 4500: 2\n",
      "loss after iteration 5000: 0.286441\n",
      "# neurons after iteration 5000: 5\n",
      "loss after iteration 5500: 0.154327\n",
      "# neurons after iteration 5500: 5\n",
      "loss after iteration 6000: 0.087877\n",
      "# neurons after iteration 6000: 5\n",
      "loss after iteration 6500: 0.053017\n",
      "# neurons after iteration 6500: 5\n",
      "loss after iteration 7000: 0.032952\n",
      "# neurons after iteration 7000: 5\n",
      "loss after iteration 7500: 0.022530\n",
      "# neurons after iteration 7500: 5\n",
      "loss after iteration 8000: 0.016771\n",
      "# neurons after iteration 8000: 5\n",
      "loss after iteration 8500: 0.013272\n",
      "# neurons after iteration 8500: 5\n",
      "loss after iteration 9000: 0.010965\n",
      "# neurons after iteration 9000: 5\n",
      "loss after iteration 9500: 0.009344\n",
      "# neurons after iteration 9500: 5\n",
      "time = 6.660475\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Numpy implementation of self-selecting MLP for binary classification\n",
    "# Author: Ryan Kingery (rkinger@g.clemson.edu)\n",
    "# Last Updated: May 2018\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import lfilter\n",
    "import time\n",
    "#from ss_perf_utils import *\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "def train_numpy(X,y,layer_dims,num_iters,lr=0.01,add_del=False):\n",
    "    sigmoid = lambda z : 1./(1+np.exp(-z))\n",
    "    \n",
    "    din,dh,dout = tuple(layer_dims)\n",
    "    m = X.shape[1]\n",
    "    delta,prob,epsilon,max_hidden_size,tau = init_add_del()\n",
    "    losses = []\n",
    "    num_neurons = []\n",
    "    \n",
    "    W1 = np.random.randn(dh,din)\n",
    "    b1 = np.random.randn(dh,1)\n",
    "    W2 = np.random.randn(dout,dh)\n",
    "    b2 = np.random.randn(dout,1)\n",
    "    \n",
    "    for t in range(num_iters):\n",
    "        # Forwardprop\n",
    "        Z1 = np.dot(W1,X)+b1\n",
    "        A = Z1.clip(min=0) # relu\n",
    "        Z2 = np.dot(W2,A)+b2\n",
    "        yhat = sigmoid(Z2).clip(1e-6,1.-1e-6)\n",
    "    \n",
    "        loss = 1./m*(-np.dot(y,np.log(yhat).T)-np.dot(1-y,np.log(1-yhat).T))\n",
    "        loss = loss.squeeze().item()\n",
    "        losses.append(loss)\n",
    "    \n",
    "        # Backprop\n",
    "        dyhat = -(np.divide(y,yhat) - np.divide(1-y, 1-yhat))\n",
    "        dZ2 = dyhat*sigmoid(Z2)*(1-sigmoid(Z2))\n",
    "        dW2 = 1./m*np.dot(dZ2,A.T)\n",
    "        db2 = 1./m*np.sum(dZ2,1,keepdims=True)\n",
    "        dA = np.dot(W2.T,dZ2)\n",
    "        dZ1 = dA\n",
    "        dZ1[Z1 < 0] = 0\n",
    "        dW1 = 1./m*np.dot(dZ1,X.T)\n",
    "        db1 = 1./m*np.sum(dZ1,1,keepdims=True)\n",
    "    \n",
    "        # gradient descent\n",
    "        W1 -= lr*dW1\n",
    "        b1 -= lr*db1\n",
    "        W2 -= lr*dW2\n",
    "        b2 -= lr*db2\n",
    "\n",
    "        if add_del and t>tau:\n",
    "            W1,b1,W2,b2 = delete_neurons_numpy(W1,b1,W2,b2,delta,prob)\n",
    "            W1,b1,W2,b2 = add_neurons_numpy(W1,b1,W2,b2,losses,epsilon,delta,\n",
    "                                              max_hidden_size,tau,prob)\n",
    "        num_neurons.append(b1.shape[0])\n",
    "\n",
    "        if t % max(1,num_iters // 20) == 0:\n",
    "            print('loss after iteration %i: %f' % (t, losses[-1]))\n",
    "            if add_del:\n",
    "                print('# neurons after iteration %i: %d' % (t, num_neurons[-1]))\n",
    "    \n",
    "    return losses,num_neurons\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_iters = 10000\n",
    "    num_samples = 1000\n",
    "    num_features = 2\n",
    "    num_hidden = 1\n",
    "    num_classes = 1\n",
    "    lr = 0.1\n",
    "    layer_dims = [num_features,num_hidden,num_classes]\n",
    "    \n",
    "    X,y,x1,x2 = gen_data(samples=num_samples,var=0.01)\n",
    "    X = X.T\n",
    "    y = y.reshape(1,-1)\n",
    "    \n",
    "    tin = time.clock()\n",
    "    losses,num_neurons = train_numpy(X,y,layer_dims,num_iters,lr=lr,add_del=True)\n",
    "    tout = time.clock()\n",
    "    tdiff = tout-tin\n",
    "    print('time = %f' % tdiff)\n",
    "    \n",
    "    #losses = np.array(losses)\n",
    "    filt_neurons = lfilter([1.0/50]*50,1,num_neurons)\n",
    "    filt_neurons[filt_neurons<1] = num_hidden\n",
    "    \n",
    "    plt.plot(losses,color='blue')\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(filt_neurons,color='green')\n",
    "    plt.title('# Neurons')\n",
    "    plt.show()    \n",
    "    \n",
    "    #plot_model(model,x1,x2)\n",
    "    #print score(model,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5e678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
